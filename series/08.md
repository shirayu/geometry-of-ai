# 第8回：時間の発見 ～一撃からの脱却～

## 注意事項

- 残差接続を「多様体上の流れ」と見る解釈は、連続時間極限での比喩的理解。
- 実際のResNetは離散的なステップであり、厳密にはODEではない。
- 「空間が安定すると時間発展が扱える」は経験的観察であり、理論的保証ではない。

## トピック

なぜ生成モデルの主流が「プロセス」を強調するようになったのか

## 内容

- **古典的生成モデルの特徴（対比として）:**
    - **GAN初期:** 潜在ベクトル $\mathbf{z}$ → 生成器 $G$ → 画像 $\mathbf{x}$（一撃）
    - **VAE初期:** エンコーダ → 潜在変数 → デコーダ（一撃）
    - 「召喚術」的な生成：中間過程の制御が弱い
- **不安定性の一因（設計上の話）:**
    - ノルムが暴れる空間では、滑らかな「軌道」が描きづらい場合がある
    - 勾配が爆発・消失し、学習が不安定になりやすい
    - 生成の途中経過を制御しづらい
- **転換点の到来（複合要因）:**
    - 正規化・残差・最適化の成熟 → 学習が安定
    - 安定した空間の上で時間発展（層を"ステップ"として読む）が扱いやすくなった
    - 残差接続、Layer Normalizationなどの技術が成熟
- **残差接続 (ResNet) の再解釈（幾何学的比喩）:**
    - 従来の理解：勾配消失問題の緩和
    - 幾何学的理解：**多様体上の情報を劣化させずに運ぶ「流れ」**
    - $\mathbf{h}_{t+1} = \mathbf{h}_t + f(\mathbf{h}_t)$
        - $\mathbf{h}_t$：現在位置
        - $f(\mathbf{h}_t)$：接空間でのベクトル場
        - 合計：連続時間の流れの離散近似として読める
    - Skip connection = 時間発展の「記憶保持機構」としても見える
- **ODE/SDE との接続:**
    - Neural ODE (2018)：連続時間での深層学習
    - $\frac{d\mathbf{h}}{dt} = f(\mathbf{h}, t, \theta)$
    - 離散的な層 → 連続的な流れ

## 実装ノート

- ResNetのskip connection（概念例）: `out = F.relu(self.conv(x)) + x`
- LayerNorm/RMSNorm等との組み合わせが安定性の鍵になることが多い

## メッセージ

空間が安定すると、「映画（プロセス）」が撮れるようになる
