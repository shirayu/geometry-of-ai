# 第6回：Transformerという測量士 ～動的な接続～

## 注意事項【重要】

- 標準Transformerの$Q,K$は単位ノルム化されていない。LayerNorm/RMSNormは$L_2$正規化とは別物。
- したがって、標準Transformerで「$Q^T K = \cos\theta$」は成り立たない（ノルムの影響が混在）。
- 「球面上のAttention」解釈が厳密に成り立つのは、Cosine AttentionやnGPTなど明示的に$L_2$正規化する設計の場合。
- それでも「角度的な見方」は、正規化設計を理解するための導線として有用。

## トピック

Attention機構の幾何学的解釈

## 内容

- **カーネル法の限界を振り返る:**
    - カーネル法（RBF, Polynomial）：固定された特徴空間への写像
    - 「地図」は入力に関わらず静的
    - 柔軟性に欠ける
- **Attentionの革新:**
    - 入力に応じて**空間自体が歪む**（ように振る舞う）
    - Query-Key-Value機構 = 動的な測量システム
- **標準Transformerの計算ステップ（厳密寄り）:**

    1. **Query ( $\mathbf{Q}$) と Key ( $\mathbf{K}$) の内積:**
        - Scaled Dot-Product: $\frac{\mathbf{Q}^T \mathbf{K}}{\sqrt{d}}$
        - **重要:** 標準Transformerでは **スケーリング**が入り、かつ **Q,Kは単位正規化が前提ではない**
        - LayerNorm/RMSNormは入っているが、これは **平均・分散の正規化**であり **単位ノルム化（L2正規化）とは別物**
        - したがって、 $\mathbf{Q}^T \mathbf{K}$ は **ノルムの影響も受ける**
    2. **球面解釈が成り立つ条件（条件付き主張）:**
        - **条件A:** $\mathbf{Q}$ と $\mathbf{K}$ が単位ノルムに正規化されている（ $|\mathbf{Q}| = |\mathbf{K}| = 1$）
        - **条件B:** この場合に限り $\mathbf{Q}^T \mathbf{K} = \cos\theta$
        - **nGPTや一部のモデル**（Cosine Attention等）がこれを満たしうる
        - **標準Transformer（一般的なBERT/GPT等）はこの条件を常には満たさない**
    3. **それでも幾何学的解釈は有用:**
        - たとえ厳密に $cos\theta$でなくても、内積は「方向の類似性」の指標として有用
        - 正規化しない場合は「方向 + 大きさ」の混合情報
        - それでも「角度中心の見方」は、**cosine attention / 正規化設計**を理解する導線として有用
- **Softmax による確率的重み付け:**
    - 内積（互換度）を確率に変換
    - 「近い（互換度が高い）ほど強く照らす」
- **Value ( $\mathbf{V}$) の加重和:**
    - 重み付き平均として情報を統合
    - 「星から星へ、光のラインで情報を運ぶ」
- **Multi-head Attention の意味:**
    - プラネタリウムに**複数の星座レイヤー**を重ね合わせる
    - 各ヘッド = 異なる「見方」（異なる部分空間への射影）
    - LDAの動的・連続版としての再解釈（比喩）
        - LDA：固定されたトピック空間
        - Multi-head：入力ごとに変化する"見方"の集合
- **RoPE（Rotary Positional Embeddings）の幾何学:**
    - 現代LLM（Llama, Qwen, Mistral等）の標準的な位置埋め込み手法
    - **核心:** 絶対位置ではなく、**回転**によって相対位置を表現
    - Query/Keyベクトルに位置依存の回転行列を適用:
        - $\mathbf{q}'_m = R_m \mathbf{q}$, $\mathbf{k}'_n = R_n \mathbf{k}$
        - 内積 $\mathbf{q}'^T_m \mathbf{k}'_n$ は $(m - n)$ のみに依存 → **相対位置**
    - **幾何学的解釈:**
        - 位置 = 球面上での回転角
        - 相対位置 = 回転角の差
        - 講義テーマ（角度・回転）と完璧に整合する設計
    - ※参考: Su et al., "RoFormer: Enhanced Transformer with Rotary Position Embedding" (2024)
- **Self-Attention = 多様体の自己測量（比喩として）:**
    - 入力系列内の各点が、他のすべての点との関係を測る
    - 「どの星とどの星を結ぶべきか」を自動発見

## 実装ノート

- 標準Attention: `torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)`
- Cosine Attention: `torch.matmul(F.normalize(Q, dim=-1), F.normalize(K, dim=-1).transpose(-2, -1))`
    - `dim=-1`の明示が重要（最後の次元で正規化）

- nGPT: すべてのベクトルが常にノルム1（設計として）

## 結論

Transformerを幾何学的に見る視点は有用だが、「どの幾何学か」は設計次第
