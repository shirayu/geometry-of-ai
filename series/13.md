# 第13回：高次元の深淵 ～幾何学的な恐怖と祝福～

## 注意事項

- MoEの「各Expertが直交部分空間を担当」は理想化された仮説であり、実際の学習済みMoEで厳密に成り立つとは限らない。
- 「赤道集中現象」は高次元球面の数学的性質であり、条件なしに成り立つ。
- 敵対的サンプルの幾何学的解釈は研究途上であり、確立された理論ではない。

## トピック

超高次元球面で何が起きているのか、そしてスパース性の活用

## 内容

- **第2回の復習と深化:**
    - 高次元では「ほぼ直交」「ほぼ等距離」
    - これは表現力の源泉になりうる
    - しかし同時に、予測不可能性の源にもなりうる
- **次元の逆説（再訪）:**
    - **赤道集中現象:** 高次元球面では「ほぼすべての点が赤道付近」
    - 数学的に：高次元極限では直感に反する集中が起きる
    - 直感：「極（北極・南極）にはほとんど誰もいない」
- **裏道（ショートカット）の存在（比喩）:**
    - 超高次元では、人間直感では追えない"近道"が存在しうる
    - モデルはそれを発見し利用する可能性がある
- **敵対的サンプル (Adversarial Examples) の幾何学:**
    - 微小ノイズで誤分類が起きる
    - 幾何学的理解（比喩）：決定境界近傍の"高次元の抜け道"

## 補論：スパース性の幾何学 ～Mixture of Expertsの世界～

高次元空間の「ほとんどが空」という性質は、呪いではなく**設計原理**として活用できる。2025年以降、大規模最先端LLMの主要アーキテクチャの一つとして台頭したMixture of Experts (MoE) は、この原理の実践例である。

※ただし、denseモデル（全パラメータを常時活性化）も依然として広く使用されており、MoEが「唯一の主流」というわけではない。用途・規模・計算資源に応じて使い分けられている。

- **MoEの基本構造:**
    - 複数の「専門家」（Expert）ネットワークを用意
    - 入力に応じて、一部の専門家だけを活性化（スパース活性化）
    - **ルーター（Router）:** どの専門家を使うかを決定するゲート機構

- **幾何学的解釈:**
    - **各Expertは「部分空間」を担当:**
        - Expert 1: 数学的推論に強い部分空間
        - Expert 2: 言語的ニュアンスに強い部分空間
        - Expert 3: コード生成に強い部分空間
        - …
    - **ルーティング = 「どの部分空間に投影するか」の動的選択**
    - 入力ベクトルと各Expert重心の角度（または内積）で決定されることが多い

- **高次元直交性との接続（第2回の伏線回収）:**
    - 高次元ではランダムベクトルはほぼ直交
    - **良いMoEの仮説:** 各Expertが担当する部分空間は、互いにほぼ直交している
    - これにより、Expert間の干渉が少なく、専門性が保たれる
    - **経験的観察:** 学習が進むと、Expert間の類似度が下がる傾向

- **スパース性の利点:**
    - **計算効率:** 全パラメータを使わずに推論（例：8つのExpertのうち2つだけ活性化）
    - **容量増大:** パラメータ数は増えるが、計算量は抑制
    - **専門化:** 各Expertが特定のタスク/ドメインに特化

- **MoEの課題と幾何学的理解:**
    - **Expert Collapse（専門家崩壊）:**
        - 一部のExpertに負荷が集中し、他が使われなくなる
        - **幾何学的解釈:** 部分空間が縮退し、次元崩壊の変種が起きる
        - 対策：Load Balancing Loss（負荷分散損失）
    - **Load Balancing Loss の幾何学:**
        - 各Expertへのルーティング確率を均等化
        - **幾何学的解釈:** 球面上の均等分布を促す正則化
        - $\mathcal{L}_{\text{balance}} = \alpha \cdot n \cdot \sum_i f_i \cdot p_i$
            - $f_i$ : Expert $i$ に割り当てられたトークンの割合
            - $p_i$ : Expert $i$ へのルーティング確率の平均
    - **Expert間の知識共有:**
        - 完全に独立だと、共通知識が重複して非効率
        - **Shared Expert:** 全入力に対して常に活性化するExpertを追加
        - **幾何学的解釈:** 共通部分空間 + 専門部分空間の階層構造

- **MoE と Multi-head Attention の対比:**

    | 機構 | 分割の単位 | 選択方式 | 幾何学的解釈 |
    | --- | --- | --- | --- |
    | Multi-head Attention | 表現次元 | 常に全ヘッド | 部分空間への同時射影 |
    | MoE | ネットワーク全体 | 動的に選択 | 部分空間への条件付き射影 |

- **最新動向（2024-2025年）:** ※2026年2月時点の代表例

    以下は急速に発展中の分野であり、状況は変化しうる点に留意されたい。

    - **Mixed-Curvature Decision Trees / Random Forests** (ICML 2025): 混合曲率（混ぜる幾何）の具体例
    - **Hyperbolic LLMs** (2025): 双曲空間でのLLM表現学習
    - **HypLoRA** (Yang et al., 2024): 双曲多様体でのLoRA適用、AQuAベンチマークで最大13.0%向上（複雑な推論タスクに有効）
    - **Hyperbolic Vision Transformers (HVT)** (Fein-Ashley et al., 2024): ポアンカレ球モデルでのViT
    - **Spectro-Riemannian GNNs** (ICLR 2025): 異なる曲率を組み合わせたグラフニューラルネット
    - **Mixtral** (Mistral AI): 8 Experts, Top-2 routing
    - **Switch Transformer** (Google): Top-1 routing でさらにスパース化
    - **DeepSeek-MoE / DeepSeek-V3**: 細粒度Expert + 共有Expert、コスト効率の高いスケーリング
    - **Soft MoE**: 離散的ルーティングを連続化（第4回補論との接続！）
    - **Llama 4**: MoEを採用（ただしLlama 3はdense）
    - ※MoE vs Denseの選択は、モデル規模・用途・推論環境により異なる

## アライメント問題の幾何学的定式化（比喩）

- **人間の「望ましい多様体」:** 倫理的制約を満たす部分
- **AIの「効率的な多様体」:** 目的関数を最小化する部分
- 問題：高次元で乖離しうる
- **MoEとの接続:** 「倫理Expert」を明示的に設けることは可能か？

## 未解決問題

- 高次元多様体の「全体像」を人間は理解できるのか？
- 説明可能性とは幾何学的に何を意味するのか？
- MoEの各Expertは「何を学んだか」を解釈できるのか？

## 警告

幾何学は中立的な道具だが、その形状は価値観を埋め込む

## 実装ノート

- 敵対的サンプル（概念例）: `x_adv = x + epsilon * sign(grad)`
- MoE（概念例、Hugging Face transformers）:

```python
from transformers import MixtralForCausalLM
model = MixtralForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-v0.1")
```

- Expert活性化パターンの可視化：ルーター出力をヒートマップ化
