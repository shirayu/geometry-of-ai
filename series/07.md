# 第7回：不確実性の復権 ～Variance Matters～

## 注意事項

- $\kappa$（集中度）を「確信度」として解釈できるのは、埋め込みをvMF分布として明示的にモデル化した場合。
- 単に$L_2$正規化して$\cos$類似度を使うだけでは、$\kappa$は自動的には得られない（点埋め込みに「霧」はない）。
- エントロピーの近似式 $H \approx -\log \kappa$ は、高次元・高$\kappa$の条件下での近似。

## トピック

「点」から「分布」へのパラダイムシフト

## 内容

- **ノルムの分離という思想:**
    - これまで：ベクトルの長さ = 混沌とした情報の塊
    - これから：意味（方向）と確信度（分散）を独立した軸に（設計として）
- **vMF分布の $\kappa$ を読み解く:**
    - $\kappa$ を「確信度」として扱えるのは、**埋め込みを vMF（または集中度付き方向分布）として明示的にモデル化し、 $\kappa$（または同等量）を推定・学習する設計を採った場合**
    - 単に L2 正規化して cos 類似を使うだけでは $\kappa$ は自動的に得られない（点埋め込みだけでは"霧"が定義されない）
    - 大きい $\kappa$ → 鋭い分布 → 「この星座しかない」（高確信）
    - 小さい $\kappa$ → ぼやけた分布 → 「複数の星座が見える」（低確信）
    - 従来の埋め込みは「点」だけを扱い、「ぼやけ」を無視していた
- **不確実性の定量化（厳密性の注意）:**
    - vMF分布のエントロピーは次元と $\kappa$ の複雑な関数（Bessel関数比などが絡む）
    - $H \approx -\log\kappa$ のような式は **高次元・高 $\kappa$** など条件付き近似として扱う
    - **教育的簡略化:** 「 $\kappa$ が大きいほど集中（低不確実性）」は正しい
- **Out-of-Distribution (OOD) 検知:**
    - ドーム上の「星のない暗闇」をどう検出するか
    - 低 $\kappa$ 領域は、**$\kappa$ を持つ設計では有力なシグナルになりうる**（設計次第）
    - 「分かりません」と言えるAIの重要性
- **ハルシネーションの幾何学（比喩）:**
    - ハルシネーション = 低 $\kappa$ 領域で無理やり星座を結ぶ行為
    - 暗闇でデタラメな線を引いてしまう
    - 対策： $\kappa$ が閾値以下なら「出力を控える」等（安全設計）
- **整列（Alignment）からの解放（条件付き）:**
    - 従来：2つの埋め込み空間を比較するには「回転合わせ（Procrustes）」が必要な場合がある
    - 今： $\kappa$ のような回転不変量を使えば座標フリーの比較がしやすい場合がある
    - 「どちらが確信度が高いか」は回転に依存しにくい

## 実装ノート

- vMFからのサンプリングはライブラリ（例：`pytorch-geometric` 等）を推奨
- 自前実装はBessel関数計算が鬼門になりがち

## メッセージ

点だけでなく、その周りの「霧」を見よ

## Visualization Break II

### デモ：Attentionの幾何学を見る（5分）

- 簡単な文章（5単語）のSelf-Attentionを（必要なら正規化版も併記して）可視化
- Query-Key の互換度をヒートマップで表示
- Attention重みが距離・角度解釈とどう対応しうるかを比較
- Multi-headで複数の「星座」が現れる様子
