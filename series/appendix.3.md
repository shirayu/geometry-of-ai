
# Appendix 3: 動的剪定の幾何学: 柔軟な回路がもたらす知能

## 導入：静的な地図から動的な回路へ

* **書くべき内容**:
* 従来のCNN等は、どんな入力でも同じ経路を通る「静的な地図」。
* Transformer以降は、入力データが計算経路を決める「動的な回路」へ変化した。

* **意図**:
* 現代の深層学習モデルが持つ「適応性」の根源を提示する。
* 次項以降の「剪定」という概念が、単なる削減ではなく「動的な選択」であることを理解させる土台を作る。

## Attention：空間内のミクロな動的枝刈り

* **書くべき内容**:
* 全トークン間の  の接続から、内積（角度）で重要な関係だけを残す操作は「ソフトな枝刈り」である。
* 幾何学的には、高次元空間で「見るべき方向」以外を遮断していると解釈できる。

* **意図**:
* (第6回) で扱った「重み付け」を、計算資源の最適化という観点から再定義する。
* MoE（マクロな剪定）へ話を繋げるための、ミクロレベルでの例示とする。

## Mixture of Experts (MoE)：マクロな部分空間スイッチング

* **書くべき内容**:
* 巨大な多様体を「部分空間（専門家）の集合」として捉え、入力に応じて使用するパラメータ領域を切り替える。
* 「全知識の動員（Dense）」と「必要な近傍の活性化（MoE）」の対比。

* **意図**:
* (第13回) の内容を、Attentionの概念をモデル全体に拡張したものとして位置づける。
* パラメータ数増大と計算量維持を両立させる「幾何学的トレードオフ」の解であることを示す。

## スパース性（疎性）の幾何学的必然

* **書くべき内容**:
* 高次元空間ではベクトル同士が「ほぼ直交（無関係）」になりやすい。
* 無関係なパラメータ（ノイズ源）を計算から外すことは、情報の純度（SNR）を高めるために数学的に必然である。

* **意図**:
* 「スパース化＝サボり」という誤解を解き、「高次元空間におけるノイズ除去」という積極的な意味付けを行う。
* 第2回・13回の「直交性」の議論を回収する。

## 効率化技術の体系化（GQA, LoRA, MoD）

* **書くべき内容**:
* **GQA**: 「視点」の冗長性を剪定（Head方向）。
* **LoRA**: 重み更新の「自由度」を剪定（ランク方向）。
* **MoD**: トークンの「通過する層」を剪定（深さ方向）。

* **意図**:
* 個別の流行技術として語られがちな手法を、「幾何学的な剪定のバリエーション」として統一的に整理する。

## FlashAttention：物理空間への幾何学的適合

* **書くべき内容**:
* 巨大な行列を小さな「タイル」に分割し、GPUの高速メモリ（SRAM）の容積に適合させる。
* 数理空間だけでなく、物理デバイスという空間へのパッキング問題としての幾何学。

* **意図**:
* アルゴリズム（ソフト）とハードウェア（物理）の両面で「空間への適合」が行われていることを示し、実装面での幾何学的視座を提供する。

## 結論：知能と抽象化

* **書くべき内容**:
* 動的剪定は「何を計算しないか（捨てるか）」を決めるプロセス。
* 情報を捨ててシグナルを研ぎ澄ませることは、「抽象化」という知能の本質そのものである。

* **意図**:
* 技術的な解説を、本講義シリーズに通底する「知能とは何か」という問いに接続して締めくくる。

## 意図

このAppendixは、講義の第6回（Attention）と第13回（MoE）を「剪定（Pruning）」という概念で接続し、現代的なAIの設計思想を統一的に理解することを目的としています。

* **パラダイムシフトの提示:**
従来のCNNなどが全入力に対して同じ経路を通る「静的な地図」であるのに対し、Transformer以降は入力データ自身が計算経路を決定する「動的な回路」に進化したことを示します。
* **AttentionとMoEの統一的解釈:**
* **Attention:** トークン間の全結合グラフから、内積（角度）に基づいて不要な関係を遮断する「ミクロな動的剪定」として再定義します。
* **MoE:** 巨大なパラメータ空間から、入力に関連する部分空間（Expert）のみを活性化する「マクロな動的剪定」として位置づけます。

* **スパース性の幾何学的必然:**
高次元空間ではベクトル同士が「ほぼ直交（無関係）」になりやすい性質があるため、無関係な次元（ノイズ）を計算から除外する「剪定」が、単なるコスト削減ではなく、情報の純度を高めるための「幾何学的な必然」であることを説きます。
* **効率化技術の体系化:**
GQA（視点の剪定）、LoRA（更新ランクの剪定）、MoD（深さの剪定）、FlashAttention（物理メモリへの幾何学的適合）といった個別の技術を、「空間の効率的な活用」という文脈で整理します。
