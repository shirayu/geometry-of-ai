# 第4回：分類の再統一 I ～Softmaxと情報幾何学～

## 注意事項

- 「ロジット＝角度」の解釈は、重みベクトルと入力ベクトルの両方が$L_2$正規化されている場合に成り立つ。
- 標準的なSoftmax分類器では、ノルムの影響も混在する。
- 情報幾何学との接続（Fisher情報＝リーマン計量）は厳密な数学的対応であり、条件なしに成り立つ。

## トピック

Softmaxの幾何学的・情報幾何学的本質

## 内容

- **（比喩として）Softmaxは「シンプレックスへの写像」である:**
    - 入力：ベクトル（しばしば正規化設計と相性が良い）
    - 出力：シンプレックス上の点（確率分布）
    - 厳密には、logit 空間から確率単体への**滑らかな写像（正規化された指数写像）**
    - 写像： $\text{softmax}(\mathbf{z})_i = \frac{\exp(z_i/\tau)}{\sum_j \exp(z_j/\tau)}$
- **最大エントロピー原理との接続:**
    - 「制約（平均値など）が与えられたとき、最もランダムな分布は何か？」
    - 答え：指数型分布族（Softmaxもその一員）
    - 球面上での制約 → vMF分布
    - シンプレックス上での制約 → カテゴリカル分布
- **情報幾何学からの視点:**
    - **フィッシャー情報行列 $F$:** 確率分布のパラメータ空間におけるリーマン計量
    - カテゴリカル分布の場合： $F_{ij} = \mathbb{E}\left[\frac{\partial \log p}{\partial \theta_i} \frac{\partial \log p}{\partial \theta_j}\right]$
    - これが「どの方向に動かすと分布が大きく変わるか」を定める
    - **自然勾配:** ユークリッド勾配ではなく、フィッシャー計量に基づく勾配
    - **自然勾配との関係（重要な注意）:**
        - AdamやRMSpropを「経験的Fisher情報の近似として解釈できる」という研究提案がある
        - ただし、これは確定的な「理論的基礎」というより **解釈の一つ**
        - 実際には仮定・近似が多く入る
        - ※関連研究: K-FAC (Martens & Grosse, 2015), SNGD (Liu et al., 2024) などが自然勾配の実用化を進めている
- **温度パラメータ $\tau$ の正体:**
    - シンプレックスへの「望遠鏡の倍率」
    - $\tau \to 0$：確信度が高い（argmax的）
    - $\tau \to \infty$：確信度が低い（一様分布的）
- **なぜロジット（logit）が角度に対応しうるのか（条件付き）:**
    - 内積 $\mathbf{w}_i^T \mathbf{x} = |\mathbf{w}_i| |\mathbf{x}| \cos\theta_i$
    - **ノルム1制約（両者が正規化）下では**： $\mathbf{w}_i^T \mathbf{x} = \cos\theta_i$
    - したがってSoftmaxは**（正規化設計なら）角度情報を確率に変換する装置**として読める

## 補論：離散と連続の界面 ～着地の幾何学～

本講義は連続多様体を主役にしているが、LLMの最終出力は**離散トークン**である。この「界面」を幾何学的に理解することは、実装上極めて重要である。

- **シンプレックスから頂点への「着地」:**
    - Softmax出力 = シンプレックス **内部** の点（連続）
    - 実際の出力 = シンプレックスの **頂点**（one-hot、離散）
    - argmax = 「最も近い頂点への射影」
    - **量子化誤差:** シンプレックス内部から頂点への距離として定量化できる

- **温度スケジューリングの幾何学:**
    - $\tau$ が高い：シンプレックス中心付近をふわふわ漂う
    - $\tau$ が低い：頂点に吸い寄せられる
    - **アニーリング:** 徐々に $\tau$ を下げる = 「軟着陸」の軌跡

- **微分可能な離散化（Gumbel-Softmax）:**
    - 問題：argmaxは微分不可能 → 勾配が流れない
    - 解決：Gumbel-Softmax（連続緩和）
    - $y_i = \frac{\exp((z_i + g_i)/\tau)}{\sum_j \exp((z_j + g_j)/\tau)}$
    - $g_i \sim \text{Gumbel}(0, 1)$：ノイズによる確率的サンプリング
    - **幾何学的解釈:** シンプレックス内部でランダムに揺らしながら、$\tau \to 0$ で頂点に収束

- **Straight-Through Estimator (STE):**
    - Forward: 離散（argmax）
    - Backward: 連続（Softmaxの勾配をそのまま使う）
    - **幾何学的トリック:** 「見かけは頂点にいるが、勾配は内部から来る」
    - これにより離散出力でも学習可能に

- **LLMにおける意味:**
    - 連続的な「思考の軌跡」（hidden states）
    - → Softmaxで確率分布に変換
    - → サンプリング or argmax で離散トークンに「着地」
    - **情報の欠落:** 連続表現が持つ微妙なニュアンスは、離散化で失われうる
    - **Top-k / Top-p サンプリング:** 「頂点の近傍」からランダムに選ぶことで多様性を確保

## 実装ノート

- 温度付きSoftmax：`torch.nn.functional.softmax(logits/temperature, dim=-1)`
- Gumbel-Softmax：`torch.nn.functional.gumbel_softmax(logits, tau=temperature, hard=False)`
- `hard=True` で STE（forward は one-hot、backward は連続）

## 準備

次回、この幾何学的視点からSVMとの統一へ
