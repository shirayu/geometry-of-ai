# Appendix 4: ç©ºé–“ã®ã€Œç‰©å·®ã—ã€å†è€ƒ: 2ç‚¹é–“ã‹ã‚‰æƒ…å ±ã®å¯†åº¦ã¾ã§

## æ³¨æ„äº‹é …

æœ¬Appendixã§æ‰±ã†å†…å®¹ã«ã¯ã€ç¢ºç«‹ã•ã‚ŒãŸæ•°å­¦çš„äº‹å®Ÿã¨ã€æ•™è‚²çš„ãªè§£é‡ˆãŒæ··åœ¨ã—ã¦ã„ã‚‹ã€‚ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«ç•™æ„ã•ã‚ŒãŸã„ï¼š

- **ã€Œè·é›¢ã€ã¨ã€Œãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€ã®åŒºåˆ¥**ï¼šæœ¬è³‡æ–™ã§ã¯æ•°å­¦çš„ã«å³å¯†ãªè·é›¢ï¼ˆå¯¾ç§°ãƒ»ä¸‰è§’ä¸ç­‰å¼ã‚’æº€ãŸã™ï¼‰ã¨ã€éå¯¾ç§°ãªã€Œéš”ãŸã‚Šã®å°ºåº¦ã€ã§ã‚ã‚‹ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æ˜ç¢ºã«åŒºåˆ¥ã™ã‚‹ã€‚KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯å¯¾ç§°æ€§ã‚’æŒãŸãªã„ãŸã‚ã€æ•°å­¦çš„ã«ã¯è·é›¢ã§ã¯ãªã„ã€‚
- **ã€Œæƒ…å ±å¯†åº¦ã€ã®ç”¨èªå®šç¾©**ï¼šæœ¬Appendixã§ç”¨ã„ã‚‹ã€Œæƒ…å ±å¯†åº¦ã€ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ã®å¯†åº¦ã‚„ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®åˆ†å¸ƒã§ã¯ãªãã€ **ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ãŒè¡¨ã™ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰åŒ–ã«å¯¾ã™ã‚‹äºˆæ¸¬åˆ†å¸ƒã®æ„Ÿåº¦ã€** ã‚’æŒ‡ã™æ•™è‚²çš„æ¯”å–©ã§ã‚ã‚‹ã€‚ã“ã®ç”¨èªã¯æ©Ÿæ¢°å­¦ç¿’æ–‡è„ˆã§è¤‡æ•°ã®æ„å‘³ã‚’æŒã¡ã†ã‚‹ãŸã‚ã€æœ¬è³‡æ–™ã§ã¯ä¸Šè¨˜ã®å®šç¾©ã«é™å®šã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
- **ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã®è§£é‡ˆ**ï¼šã€Œç©ºé–“ã®å¯†åº¦ã€ã€Œã‚¸ãƒ£ãƒ³ã‚°ãƒ«ã¨ç ‚æ¼ ã€ã¯æ•™è‚²çš„ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã§ã‚ã‚Šã€å³å¯†ã«ã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬åˆ†å¸ƒã®æ„Ÿåº¦ã€ã‚’è¡¨ã™è¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ«ã§ã‚ã‚‹ã€‚
- **è‡ªç„¶å‹¾é…æ³•ã®åŠ¹æœ**ï¼šè‡ªç„¶å‹¾é…æ³•ãŒã€Œå¸¸ã«é€šå¸¸å‹¾é…ã‚ˆã‚Šå„ªã‚Œã‚‹ã€ã‚ã‘ã§ã¯ãªã„ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã€è¿‘ä¼¼ç²¾åº¦ã€ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã«ã‚ˆã‚Šå®Ÿç”¨æ€§ãŒå¤‰ã‚ã‚‹ã€‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯è¿‘ä¼¼æ‰‹æ³•ï¼ˆK-FACç­‰ï¼‰ãŒå¿…è¦ã€‚
- **æƒ…å ±å¯†åº¦ã¨Attention/MoEã®é–¢ä¿‚**ï¼šã€ŒAttentionãŒæƒ…å ±å¯†åº¦ã®é«˜ã„å ´æ‰€ã«é‡ã¿ã‚’ç½®ãã€ã¨ã„ã†è¨˜è¿°ã¯ã€ç›´æ„Ÿçš„ç†è§£ã‚’åŠ©ã‘ã‚‹ãŸã‚ã®**æ¯”å–©çš„å¯¾å¿œã¥ã‘**ã§ã‚ã‚‹ã€‚Attentionã¯å†…ç©å¹¾ä½•ã«åŸºã¥ãé¡ä¼¼åº¦è¨ˆç®—ã€Fisherã¯çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã®è¨ˆé‡ã§ã‚ã‚Šã€å³å¯†ã«ã¯åˆ¥æ¦‚å¿µã§ã‚ã‚‹ã€‚ã¾ãŸã€å› æœé–¢ä¿‚ã¯åŒæ–¹å‘çš„ã§ã€å­¦ç¿’ã«ã‚ˆã£ã¦ç‰¹å®šé ˜åŸŸã®æƒ…å ±å¯†åº¦ï¼ˆæ„Ÿåº¦ï¼‰ãŒ**å½¢æˆã•ã‚Œã‚‹**å´é¢ã‚‚ã‚ã‚‹ã€‚

## å°å…¥ï¼šã€Œç‰©å·®ã—ã€ãŒä¸–ç•Œã‚’å®šç¾©ã™ã‚‹

### å¹¾ä½•å­¦ã¨ã¯ã€Œæ¸¬ã‚‹ã€ã“ã¨

ç©ºé–“ã‚’ç†è§£ã™ã‚‹ã¨ã¯ã€ãã“ã§ä½¿ãˆã‚‹ **ã€Œç‰©å·®ã—ï¼ˆè¨ˆé‡ã€metricï¼‰ã€** ã‚’çŸ¥ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

å°å­¦æ ¡ã§ç¿’ã†ç®—æ•°ã§ã¯ã€2ç‚¹é–“ã®è·é›¢ã¯å®šè¦ã§æ¸¬ã‚‹ã€‚ã—ã‹ã—ã€ãã®ã€Œå®šè¦ã€è‡ªä½“ãŒã€ã©ã®ã‚ˆã†ãªç©ºé–“ã«ã„ã‚‹ã‹ã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ã€‚å¹³ã‚‰ãªç´™ã®ä¸Šã§ã¯ç›´ç·šè·é›¢ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰ãŒè‡ªç„¶ã ãŒã€åœ°çƒå„€ã®ä¸Šã§ã¯å¤§å††ã«æ²¿ã£ãŸæ¸¬åœ°è·é›¢ãŒé©åˆ‡ã ã€‚

æ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹è¡¨ç¾ç©ºé–“ã‚‚åŒæ§˜ã§ã‚ã‚‹ã€‚ãƒ™ã‚¯ãƒˆãƒ«ã®ã€Œè¿‘ã•ã€ã‚’æ¸¬ã‚‹æ–¹æ³•ã¯ä¸€ã¤ã§ã¯ãªã„ï¼š

| æ¸¬ã‚Šæ–¹ | ä½•ã‚’æ¸¬ã‚‹ã‹ | ç‰¹æ€§ | æ·±å±¤å­¦ç¿’ã§ã®ä¾‹ |
| --- | --- | --- | --- |
| **ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢** | 2ç‚¹é–“ã®ç›´ç·šè·é›¢ | å¯¾ç§°ã€ä¸‰è§’ä¸ç­‰å¼ã‚’æº€ãŸã™ | Word2Vecï¼ˆåˆæœŸï¼‰ã€PCA |
| **å†…ç©ï¼ˆdot productï¼‰** | ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã¨æ–¹å‘ | å¯¾ç§° | Attentionï¼ˆscaled dot-productï¼‰ |
| **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦** | ãƒ™ã‚¯ãƒˆãƒ«ã®æ–¹å‘ã®è¿‘ã• | å¯¾ç§°ã€é•·ã•ã‚’ç„¡è¦– | æ­£è¦åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ï¼ˆL2æ­£è¦åŒ–å¾Œã®å†…ç©ï¼‰ |
| **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** | åˆ†å¸ƒã®ã€Œéš”ãŸã‚Šã€ | éå¯¾ç§° | æå¤±é–¢æ•°ã€VAE |
| **ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±** | åˆ†å¸ƒã®å¤‰åŒ–ã—ã‚„ã™ã•ï¼ˆæ„Ÿåº¦ï¼‰ | è¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ« | è‡ªç„¶å‹¾é…æ³•ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å¹¾ä½• |

### AIã«ã¨ã£ã¦ã®å¤šå±¤çš„ãªå®šè¦

AIã«ã¨ã£ã¦ã®ç©ºé–“ã¯ã€å˜ãªã‚‹ç‰©ç†çš„ãªã€Œé•·ã•ã€ã ã‘ã§ãªãã€ä»¥ä¸‹ã®ã‚ˆã†ãªå¤šå±¤çš„ãªå®šè¦ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ï¼š

1. **é…ç½®ã®å®šè¦**ï¼šã©ã“ã«ç‚¹ã‚’ç½®ãã‹ï¼ˆå¯¾ç§°çš„ãªè·é›¢ãƒ»å†…ç©ï¼‰
2. **å­¦ç¿’ã®å®šè¦**ï¼šã©ã®æ–¹å‘ã«é€²ã‚€ã‹ï¼ˆéå¯¾ç§°ãªãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼‰
3. **æ„Ÿåº¦ã®å®šè¦**ï¼šç©ºé–“ã®ã©ã“ãŒã€Œç¡¬ã„ã€ã‹ï¼ˆãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ï¼‰

æœ¬Appendixã§ã¯ã€ã“ã‚Œã‚‰3ã¤ã®è¦–ç‚¹ã‹ã‚‰ã€Œç‰©å·®ã—ã€ã‚’å†è€ƒã—ã€æƒ…å ±å¹¾ä½•å­¦ã®æ ¸å¿ƒçš„ãªæ¦‚å¿µã‚’ã€æ•°å¼ã‚’æœ€å°é™ã«æŠ‘ãˆã¤ã¤ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚

> [!TIP]
> **èª­è€…ã®å¹¾ä½•å­¦çš„ç›´æ„Ÿã®æ‹¡å¼µ**ï¼šç‰©ç†çš„ãªã€Œé•·ã•ã€ã¨ã„ã†ç›´æ„Ÿã‹ã‚‰ã€æƒ…å ±çš„ãªã€Œé‡ã€ã¸ã¨è¦–é‡ã‚’åºƒã’ã‚‹ã“ã¨ãŒæœ¬Appendixã®ç›®æ¨™ã§ã‚ã‚‹ã€‚è·é›¢ã‚’æ¸¬ã‚‹ã“ã¨ã¯ã€å˜ã«ã€Œã©ã‚Œã ã‘é›¢ã‚Œã¦ã„ã‚‹ã‹ã€ã‚’çŸ¥ã‚‹ã ã‘ã§ãªãã€ã€Œã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã€ã€Œã©ã“ãŒé‡è¦ã‹ã€ã‚’çŸ¥ã‚‹ã“ã¨ã§ã‚‚ã‚ã‚‹ã€‚

## å¯¾ç§°çš„ãªå°ºåº¦ï¼ˆè·é›¢ï¼‰ï¼šé…ç½®ã¨æ§‹é€ ã®å›ºå®š

### å†…ç©ï¼šæ–¹å‘ã¨å¤§ãã•ã‚’æ¸¬ã‚‹åŸºæœ¬é“å…·

æœ€ã‚‚åŸºæœ¬çš„ãªã€Œæ¸¬ã‚Šæ–¹ã€ã¯ **å†…ç©ï¼ˆinner product / dot productï¼‰** ã§ã‚ã‚‹ã€‚2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{u}, \mathbf{v}$ ã®å†…ç©ã¯ï¼š

$$\mathbf{u} \cdot \mathbf{v} = \sum_i u_i v_i = \|\mathbf{u}\| \|\mathbf{v}\| \cos\theta$$

ã“ã®å¼ã¯ã€å†…ç©ãŒä»¥ä¸‹ã®2ã¤ã®æƒ…å ±ã‚’å«ã‚€ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ï¼š

- **å¤§ãã•**ï¼ˆãƒãƒ«ãƒ  $\|\mathbf{u}\|, \|\mathbf{v}\|$ ï¼‰
- **æ–¹å‘ã®è¿‘ã•**ï¼ˆè§’åº¦ $\theta$ ã®ã‚³ã‚µã‚¤ãƒ³ï¼‰

### ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼šæ–¹å‘ã ã‘ã‚’è¦‹ã‚‹

ä¸¡æ–¹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆå˜ä½é•·ã«ï¼‰ã™ã‚‹ã¨ã€å†…ç©ã¯ç´”ç²‹ã«æ–¹å‘ã ã‘ã‚’æ¸¬ã‚‹ **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦** ã«ãªã‚‹ï¼š

$$\text{cosine similarity} = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \cos\theta$$

ã“ã‚Œã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ï¼ˆãƒãƒ«ãƒ ï¼‰ã‚’ç„¡è¦–ã—ã€å‘ãã ã‘ã«æ³¨ç›®ã™ã‚‹å°ºåº¦ã§ã‚ã‚‹ã€‚

### Attentionã¨ã®é–¢ä¿‚

ç¬¬6å›ã§æ‰±ã£ãŸAttentionæ©Ÿæ§‹ï¼ˆæ¨™æº–çš„ãªscaled dot-product attentionï¼‰ã¯ã€Queryã¨Keyã® **å†…ç©ï¼ˆdot productï¼‰** ã‚’åŸºæœ¬ã¨ã—ã¦ã„ã‚‹ï¼š

$$\text{score}(q_i, k_j) = \frac{q_i^\top k_j}{\sqrt{d_k}}$$

ã“ã®å†…ç©ã¯ã€**ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã¨æ–¹å‘ã®ä¸¡æ–¹**ã‚’åæ˜ ã™ã‚‹ã€‚ãŸã ã—ã€å®Ÿè£…ã«ã‚ˆã£ã¦ã¯ï¼š

- **L2æ­£è¦åŒ–ã•ã‚ŒãŸAttention**ï¼šQuery/Keyã‚’äº‹å‰ã«æ­£è¦åŒ–ã™ã‚Œã°ã€å†…ç©ã¯ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨ç­‰ä¾¡ã«ãªã‚‹
- **æ¨™æº–çš„ãªAttention**ï¼šæ­£è¦åŒ–ãªã—ã§ã¯ã€å†…ç©ã¯ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã‚‚è€ƒæ…®ã™ã‚‹

```appendix4_cosine_similarity.py
import torch


def dot_product(u, v):
    """å†…ç©ã®è¨ˆç®—ï¼ˆæ¨™æº–çš„ãªAttentionã«ç›¸å½“ï¼‰"""
    return torch.dot(u, v)


def cosine_similarity(u, v):
    """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—ï¼ˆæ­£è¦åŒ–å¾Œã®å†…ç©ï¼‰"""
    u_norm = u / u.norm()
    v_norm = v / v.norm()
    return torch.dot(u_norm, v_norm)


# ä¾‹ï¼šåŒã˜æ–¹å‘ã ãŒå¤§ãã•ãŒç•°ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«
u = torch.tensor([1.0, 2.0, 3.0])
v = torch.tensor([2.0, 4.0, 6.0])  # uã®2å€

print(f"å†…ç©: {dot_product(u, v):.2f}")
print(f"ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {cosine_similarity(u, v):.4f}")
# å†…ç©: 28.00  â† å¤§ãã•ã‚‚åæ˜ ã•ã‚Œã‚‹
# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: 1.0000  â† å®Œå…¨ã«åŒã˜æ–¹å‘ï¼ˆå¤§ãã•ã¯ç„¡è¦–ï¼‰
```

> [!NOTE]
> **ç¬¬6å›ã¨ã®æ¥ç¶š**ï¼šæ¨™æº–çš„ãªAttentionã¯å†…ç©ã‚’ä½¿ç”¨ã—ã€Softmaxã§æ­£è¦åŒ–ã™ã‚‹ã€‚ã“ã‚Œã¯ã€Œã©ã®KeyãŒQueryã¨é«˜ã„å†…ç©ã‚’æŒã¤ã‹ã€ã‚’æ¸¬å®šã—ã€ãã®å€¤ã«å¿œã˜ã¦æƒ…å ±ã‚’é›†ç´„ã™ã‚‹æ“ä½œã§ã‚ã‚‹ã€‚å†…ç©ã¯ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã‚‚è€ƒæ…®ã™ã‚‹ãŸã‚ã€ç´”ç²‹ãªã€Œæ–¹å‘ã®é¡ä¼¼åº¦ã€ã¨ã¯ç•°ãªã‚‹ã€‚ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼šnGPTãªã©ï¼‰ã§ã¯æ˜ç¤ºçš„ã«L2æ­£è¦åŒ–ã‚’è¡Œã„ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®Attentionã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚

### åŒæ›²è·é›¢ï¼šéšå±¤æ§‹é€ ã®æ·±ã•ã‚’æ¸¬ã‚‹

ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã‚„çƒé¢ç©ºé–“ã¨ã¯ç•°ãªã‚‹è¨ˆé‡ã¨ã—ã¦ã€**åŒæ›²è·é›¢ï¼ˆhyperbolic distanceï¼‰** ãŒã‚ã‚‹ã€‚ã“ã‚Œã¯è² ã®æ›²ç‡ã‚’æŒã¤ç©ºé–“ã§ã®è·é›¢ã§ã‚ã‚‹ã€‚

ç¬¬12å›ã§è©³ã—ãæ‰±ã£ãŸãŒã€åŒæ›²ç©ºé–“ã®é‡è¦ãªç‰¹å¾´ã¯ï¼š

- **ä¸­å¿ƒã‹ã‚‰é›¢ã‚Œã‚‹ã»ã©ç©ºé–“ãŒæŒ‡æ•°çš„ã«åºƒãŒã‚‹**
- **éšå±¤æ§‹é€ ï¼ˆæœ¨æ§‹é€ ï¼‰ã®åŸ‹ã‚è¾¼ã¿ã«é©ã—ã¦ã„ã‚‹**

ä¾‹ãˆã°ã€PoincarÃ©çƒãƒ¢ãƒ‡ãƒ«ã§ã¯ã€è·é›¢ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼š

$$d(\mathbf{u}, \mathbf{v}) = \text{arcosh}\left(1 + 2\frac{\|\mathbf{u} - \mathbf{v}\|^2}{(1-\|\mathbf{u}\|^2)(1-\|\mathbf{v}\|^2)}\right)$$

ã“ã®è·é›¢ã¯ã€ä¸­å¿ƒï¼ˆåŸç‚¹ï¼‰ã®è¿‘ãã§ã¯ã€Œè¦ªã€ã‚’è¡¨ã—ã€å¢ƒç•Œã«è¿‘ã„ã»ã©ã€Œæ·±ã„å­å­«ã€ã‚’è¡¨ç¾ã§ãã‚‹ã€‚

> [!NOTE]
> **å®Ÿè£…ä¸Šã®æ³¨æ„**ï¼šPoincarÃ©çƒãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ã™ã¹ã¦ã®ç‚¹ãŒ $\|\mathbf{u}\| < 1$ ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚‹ï¼ˆé–‹çƒå†…ï¼‰ã€‚å®Ÿè£…æ™‚ã«ã¯æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€ä»¥ä¸‹ã®å¯¾ç­–ãŒæ¨å¥¨ã•ã‚Œã‚‹ï¼š
>
> 1. **å¢ƒç•Œã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**ï¼šå¢ƒç•Œ $\|\mathbf{u}\| = 1$ ã«è¿‘ã¥ãã™ããªã„ã‚ˆã†ã€ä¾‹ãˆã° $\|\mathbf{u}\| < 1 - \epsilon$ ï¼ˆ $\epsilon = 10^{-5}$ ï¼‰ã§ã‚¯ãƒªãƒƒãƒ—
> 2. **arcoshã®å¼•æ•°ã®ä¿è¨¼**ï¼šæµ®å‹•å°æ•°ç‚¹èª¤å·®ã§ arcosh ã®å¼•æ•°ãŒ 1 æœªæº€ã«ãªã‚Šå¾—ã‚‹ãŸã‚ã€`arg = arg.clamp(min=1 + eps)` ã®ã‚ˆã†ãªå‡¦ç†ãŒæœ‰ç”¨
> 3. **ã‚¼ãƒ­é™¤ç®—å›é¿**ï¼šåˆ†æ¯ $(1-\|\mathbf{u}\|^2)(1-\|\mathbf{v}\|^2)$ ãŒã‚¼ãƒ­ã«ãªã‚‰ãªã„ã‚ˆã†ã€ä¸Šè¨˜ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¨åˆã‚ã›ã¦å¯¾ç­–ã™ã‚‹

| ç©ºé–“ | æ›²ç‡ | è·é›¢ã®ç‰¹å¾´ | é©ã—ãŸæ§‹é€  | æ·±å±¤å­¦ç¿’ã§ã®ä¾‹ |
| --- | --- | --- | --- | --- |
| **ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰** | 0 | ä¸€æ§˜ | ç‰¹å®šã®æ§‹é€ ãªã— | PCAã€åˆæœŸã®Word2Vec |
| **çƒé¢** | æ­£ | ä¸­å¿ƒã¸ã®å›å¸°æ€§ | æ–¹å‘ã®å¤šæ§˜æ€§ | æ­£è¦åŒ–åŸ‹ã‚è¾¼ã¿ï¼ˆç¬¬3å›ï¼‰ |
| **åŒæ›²** | è²  | å¤–ã«å‘ã‹ã£ã¦æ‹¡å¼µ | éšå±¤ãƒ»æœ¨æ§‹é€  | PoincarÃ© Embeddingï¼ˆç¬¬12å›ï¼‰ |

> [!IMPORTANT]
> **é™çš„ãªé…ç½®ã¨ã—ã¦ã®è·é›¢**ï¼šã“ã‚Œã‚‰ã®è·é›¢ï¼ˆå†…ç©ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã€åŒæ›²è·é›¢ï¼‰ã¯ã™ã¹ã¦ **å¯¾ç§°çš„** ã§ã‚ã‚‹ã€‚ã¤ã¾ã‚Šã€ç‚¹Aã‹ã‚‰ç‚¹Bã¸ã®è·é›¢ã¨ã€ç‚¹Bã‹ã‚‰ç‚¹Aã¸ã®è·é›¢ãŒç­‰ã—ã„ã€‚ã“ã‚Œã‚‰ã¯ **ã€Œã©ã“ã«é…ç½®ã™ã‚‹ã‹ã€ã‚’æ±ºã‚ã‚‹é™çš„ãªåœ°å›³ï¼ˆStatic Mapï¼‰** ã®ãŸã‚ã®å®šè¦ã§ã‚ã‚‹ã€‚ã—ã‹ã—ã€å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼‰ã®æ–¹å‘æ€§ã‚’æ±ºã‚ã‚‹ã«ã¯ã€éå¯¾ç§°ãªå°ºåº¦ãŒå¿…è¦ã«ãªã‚‹ã€‚

> [!NOTE]
> **Attentionã¨å†…ç©**ï¼šç¬¬6å›ã§æ‰±ã†æ¨™æº–çš„ãªAttentionï¼ˆscaled dot-product attentionï¼‰ã¯ã€å†…ç©ã‚’åŸºæœ¬ã¨ã™ã‚‹ã€‚å†…ç©ã¯ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã¨æ–¹å‘ã®ä¸¡æ–¹ã‚’åæ˜ ã™ã‚‹ãŸã‚ã€ç´”ç²‹ãªã€Œæ–¹å‘ã®é¡ä¼¼åº¦ã€ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ã¨ã¯ç•°ãªã‚‹ã€‚ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆnGPTãªã©ï¼‰ã§ã¯æ˜ç¤ºçš„ã«L2æ­£è¦åŒ–ã‚’è¡Œã„ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®Attentionã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚

## éå¯¾ç§°ãªå°ºåº¦ï¼ˆãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼‰ï¼šå­¦ç¿’ã®é§†å‹•åŠ›

### è·é›¢ã§ã¯å­¦ç¿’ã®æ–¹å‘ãŒè¦‹ãˆãªã„

å¯¾ç§°çš„ãªè·é›¢ã¯ã€2ç‚¹é–“ã®ã€Œéš”ãŸã‚Šã€ã‚’æ¸¬ã‚‹ãŒã€**ã©ã¡ã‚‰ã‹ã‚‰ã©ã¡ã‚‰ã«å‘ã‹ã†ã¹ãã‹**ã¨ã„ã†æ–¹å‘æ€§ã¯æŒãŸãªã„ã€‚

å­¦ç¿’ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®åˆ†å¸ƒ $Q$ ã‚’çœŸã®åˆ†å¸ƒ $P$ ã« **è¿‘ã¥ã‘ã‚‹** ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚ã‚‹ã€‚ã“ã®ã€Œè¿‘ã¥ãã€ã¨ã„ã†æ–¹å‘æ€§ã‚’è¡¨ç¾ã™ã‚‹ã«ã¯ã€éå¯¾ç§°ãªå°ºåº¦ãŒå¿…è¦ã ã€‚

### KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼šåˆ†å¸ƒã®éå¯¾ç§°ãªéš”ãŸã‚Š

**KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆKullback-Leibler divergenceï¼‰** ã¯ã€2ã¤ã®ç¢ºç‡åˆ†å¸ƒ $P$ ã¨ $Q$ ã®ã€Œéš”ãŸã‚Šã€ã‚’æ¸¬ã‚‹ãŒã€è¡Œãã¨å¸°ã‚Šã§å€¤ãŒç•°ãªã‚‹ï¼š

$$D_{\text{KL}}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$$

$$D_{\text{KL}}(Q \| P) = \sum_x Q(x) \log \frac{Q(x)}{P(x)}$$

ä¸€èˆ¬ã«ã€ $D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$ ã§ã‚ã‚‹ã€‚

### éå¯¾ç§°æ€§ã®å¹¾ä½•å­¦çš„æ„å‘³

ãªãœéå¯¾ç§°ãªã®ã‹ï¼Ÿ ãã‚Œã¯ã€**ã€Œã©ã¡ã‚‰ã®è¦–ç‚¹ã§æ¸¬ã‚‹ã‹ã€ãŒç•°ãªã‚‹**ã‹ã‚‰ã§ã‚ã‚‹ã€‚

- $D_{\text{KL}}(P \| Q)$ ï¼šã€ŒçœŸã®åˆ†å¸ƒ $P$ ã‹ã‚‰è¦‹ã¦ã€ãƒ¢ãƒ‡ãƒ« $Q$ ãŒã©ã‚Œã ã‘ã‚ºãƒ¬ã¦ã„ã‚‹ã‹ã€
    - $P$ ã®ç¢ºç‡ãŒé«˜ã„å ´æ‰€ã§ $Q$ ã®ç¢ºç‡ãŒä½ã„ã¨ã€å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
    - **Forward KL** ã¨ã‚‚å‘¼ã°ã‚Œã‚‹
- $D_{\text{KL}}(Q \| P)$ ï¼šã€Œãƒ¢ãƒ‡ãƒ« $Q$ ã‹ã‚‰è¦‹ã¦ã€çœŸã®åˆ†å¸ƒ $P$ ãŒã©ã‚Œã ã‘ã‚ºãƒ¬ã¦ã„ã‚‹ã‹ã€
    - $Q$ ã®ç¢ºç‡ãŒé«˜ã„å ´æ‰€ã§ $P$ ã®ç¢ºç‡ãŒä½ã„ã¨ã€å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
    - **Reverse KL** ã¨ã‚‚å‘¼ã°ã‚Œã‚‹

```appendix4_kl_asymmetry.py
import torch

# çœŸã®åˆ†å¸ƒ P ã¨ ãƒ¢ãƒ‡ãƒ« Q
P = torch.tensor([0.1, 0.6, 0.3])
Q = torch.tensor([0.4, 0.3, 0.3])


# KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®è¨ˆç®—ï¼ˆæ‰‹è¨ˆç®—ç‰ˆï¼‰
def kl_divergence(p, q):
    """KL(P||Q) ã‚’è¨ˆç®—"""
    return (p * torch.log(p / q)).sum()


kl_pq = kl_divergence(P, Q)
kl_qp = kl_divergence(Q, P)

print(f"KL(P||Q): {kl_pq:.4f}")
print(f"KL(Q||P): {kl_qp:.4f}")
print(f"éå¯¾ç§°æ€§: {abs(kl_pq - kl_qp):.4f}")
# å‡ºåŠ›ä¾‹:
# KL(P||Q): 0.2332
# KL(Q||P): 0.1596
# éå¯¾ç§°æ€§: 0.0736
```

> [!NOTE]
> **PyTorchã®å®Ÿè£…**ï¼šPyTorchã® `F.kl_div` ã¯å†…éƒ¨ã§å¯¾æ•°ã‚’å–ã‚‹ãŸã‚ã€å¼•æ•°ã®é †åºã«æ³¨æ„ãŒå¿…è¦ã€‚`F.kl_div(Q.log(), P, reduction='sum')` ã¯ $D_{\text{KL}}(P \| Q)$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚

### éå¯¾ç§°æ€§ãŒå­¦ç¿’ã‚’é§†å‹•ã™ã‚‹

ã“ã®éå¯¾ç§°æ€§ã“ããŒã€**æå¤±é–¢æ•°ã¨ã—ã¦å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼‰ã‚’é§†å‹•ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼**ã¨ãªã‚‹ã€‚

ä¾‹ãˆã°ã€ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã¯ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ã„ã‚‹ï¼š

$$\mathcal{L} _{\text{CE}} = -\sum _x P(x) \log Q(x) = D _{\text{KL}}(P \| Q) + H(P)$$

ã“ã“ã§ $H(P)$ ã¯ $P$ ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆå®šæ•°ï¼‰ã€‚ã¤ã¾ã‚Šã€ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯ã€Forward KL $D_{\text{KL}}(P \| Q)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã€‚

| å°ºåº¦ | å¯¾ç§°æ€§ | ç”¨é€” | è¬›ç¾©ã§ã®ä½ç½® |
| --- | --- | --- | --- |
| **è·é›¢ï¼ˆå†…ç©ãƒ»ã‚³ã‚µã‚¤ãƒ³ï¼‰** | å¯¾ç§° | é…ç½®ã®æ±ºå®š | ç¬¬6å›ï¼ˆAttentionï¼‰ |
| **åŒæ›²è·é›¢** | å¯¾ç§° | éšå±¤æ§‹é€ ã®è¡¨ç¾ | ç¬¬12å› |
| **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** | éå¯¾ç§° | å­¦ç¿’ã®é§†å‹• | ç¬¬4å›ï¼ˆSoftmaxï¼‰ |

> [!IMPORTANT]
> **ç¬¬4å›ã¨ã®æ¥ç¶š**ï¼šç¬¬4å›ã§æ‰±ã£ãŸSoftmax/Cross-Entropyã®å¹¾ä½•å­¦çš„æ„å‘³ã¯ã€ã¾ã•ã«ã“ã®éå¯¾ç§°ãªKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«ã‚ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ã®åˆ†å¸ƒã‚’çœŸã®åˆ†å¸ƒã«ã€Œå¼•ãå¯„ã›ã‚‹ã€åŠ›ãŒã€å­¦ç¿’ã®å‹¾é…ã¨ãªã‚‹ã€‚å¯¾ç§°çš„ãªè·é›¢ã§ã¯ã€ã“ã®å¼•åŠ›ã®æ–¹å‘ãŒå®šç¾©ã§ããªã„ã€‚

## ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ï¼šç©ºé–“ã®ã€Œå¯†åº¦ã€ã¨ã€Œæ„Ÿåº¦ã€

### ç©ºé–“ã¯ä¸€æ§˜ã§ã¯ãªã„ï¼šæ›²ãŒã‚Šå…·åˆã®å®šé‡åŒ–

ã“ã‚Œã¾ã§è¦‹ã¦ããŸè·é›¢ã‚„ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ã€**å€‹åˆ¥ã®2ç‚¹é–“**ã®éš”ãŸã‚Šã‚’æ¸¬ã‚‹ã‚‚ã®ã ã£ãŸã€‚ã—ã‹ã—ã€ç©ºé–“å…¨ä½“ã‚’è¦‹æ¸¡ã™ã¨ã€**å ´æ‰€ã«ã‚ˆã£ã¦ã€Œç¡¬ã•ã€ã‚„ã€Œå¯†åº¦ã€ãŒç•°ãªã‚‹**ã“ã¨ãŒã‚ã‚‹ã€‚

å¹³ã‚‰ãªç´™ã®ä¸Šã§ã¯ã€ã©ã“ã‚’æ­©ã„ã¦ã‚‚åŒã˜åŠ´åŠ›ã§é€²ã‚ã‚‹ã€‚ã—ã‹ã—ã€å±±å²³åœ°å¸¯ã§ã¯ã€æ€¥ãªå‚ã‚’ç™»ã‚‹ã®ã¯å¤§å¤‰ã ãŒã€å¹³å¦ãªé“ã¯æ¥½ã«é€²ã‚ã‚‹ã€‚

ç¢ºç‡åˆ†å¸ƒã®ç©ºé–“ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ï¼‰ã‚‚åŒæ§˜ã§ã‚ã‚‹ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°‘ã—å‹•ã‹ã—ãŸã¨ãã€äºˆæ¸¬åˆ†å¸ƒãŒ **æ¿€å¤‰ã™ã‚‹å ´æ‰€** ã¨ã€**ã»ã¨ã‚“ã©å¤‰ã‚ã‚‰ãªã„å ´æ‰€** ãŒã‚ã‚‹ã€‚

### ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ï¼šè¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦ã®å®šç¾©

**ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ï¼ˆFisher Information Matrixï¼‰** ã¯ã€ã“ã®ã€Œç©ºé–“ã®æ›²ãŒã‚Šå…·åˆã€ã‚’å®šé‡åŒ–ã™ã‚‹ï¼š

$$G_{ij}(\theta) = \mathbb{E}_{p(x;\theta)}\left[\frac{\partial \log p(x;\theta)}{\partial \theta_i} \frac{\partial \log p(x;\theta)}{\partial \theta_j}\right]$$

ã“ã‚Œã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å„ç‚¹ã«ãŠã‘ã‚‹ **è¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆmetric tensorï¼‰** ã§ã‚ã‚‹ã€‚

> [!NOTE]
> **æ•°å­¦çš„èƒŒæ™¯**ï¼šãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã¯ã€ç¢ºç‡åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã«ãƒªãƒ¼ãƒãƒ³è¨ˆé‡ã‚’å°å…¥ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãŒã€Œæ›²ãŒã£ãŸç©ºé–“ï¼ˆãƒªãƒ¼ãƒãƒ³å¤šæ§˜ä½“ï¼‰ã€ã«ãªã‚‹ã€‚ã“ã®è¨ˆé‡ã‚’ä½¿ã†ã¨ã€åˆ†å¸ƒã®ã€Œæœ¬è³ªçš„ãªå¤‰åŒ–é‡ã€ã‚’æ­£ã—ãæ¸¬ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### ç›´æ„Ÿçš„è§£é‡ˆï¼šã‚¸ãƒ£ãƒ³ã‚°ãƒ«ã¨ç ‚æ¼ 

æ•°å¼ã‚’é›¢ã‚Œã¦ã€ç›´æ„Ÿçš„ã«ç†è§£ã—ã‚ˆã†ã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ã€èµ·ä¼ã®ã‚ã‚‹åœ°å½¢ã¨ã—ã¦æƒ³åƒã™ã‚‹ï¼š

- **ã‚¸ãƒ£ãƒ³ã‚°ãƒ«ï¼ˆæƒ…å ±å¯†åº¦ãŒé«˜ã„ï¼‰**ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°‘ã—å‹•ã‹ã™ã¨ã€äºˆæ¸¬åˆ†å¸ƒãŒå¤§ããå¤‰ã‚ã‚‹å ´æ‰€ã€‚ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±ãŒå¤§ãã„ã€‚
- **ç ‚æ¼ ï¼ˆæƒ…å ±å¯†åº¦ãŒä½ã„ï¼‰**ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ã‚‚ã€äºˆæ¸¬åˆ†å¸ƒãŒã»ã¨ã‚“ã©å¤‰ã‚ã‚‰ãªã„å ´æ‰€ã€‚ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±ãŒå°ã•ã„ã€‚

```txt
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®åœ°å½¢ï¼ˆæ¦‚å¿µå›³ï¼‰:

    ã‚¸ãƒ£ãƒ³ã‚°ãƒ«ï¼ˆæ€¥å³»ï¼‰    ç ‚æ¼ ï¼ˆå¹³å¦ï¼‰
    â›°ï¸  â›°ï¸  â›°ï¸          ğŸœï¸ ğŸœï¸ ğŸœï¸
    æƒ…å ±å¯†åº¦: é«˜         æƒ…å ±å¯†åº¦: ä½
    å°‘ã—ã®ç§»å‹•ã§åˆ†å¸ƒãŒæ¿€å¤‰   å¤§ããç§»å‹•ã—ã¦ã‚‚åˆ†å¸ƒã¯ã»ã¼ä¸å¤‰
```

ã“ã®ã€Œåœ°å½¢ã€ã‚’çŸ¥ã‚‹ã“ã¨ã¯ã€åŠ¹ç‡çš„ãªå­¦ç¿’ã«ã¨ã£ã¦æœ¬è³ªçš„ã«é‡è¦ã§ã‚ã‚‹ã€‚

### å…·ä½“ä¾‹ï¼šæ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

æ­£è¦åˆ†å¸ƒ $N(\mu, \sigma^2)$ ã‚’è€ƒãˆã‚ˆã†ã€‚ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã¯ï¼š

$$G = \begin{pmatrix} 1/\sigma^2 & 0 \\ 0 & 2/\sigma^4 \end{pmatrix}$$

ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã‹ï¼Ÿ

- **å¹³å‡ $\mu$ ã®æ„Ÿåº¦**ï¼š $1/\sigma^2$ ã«æ¯”ä¾‹ã€‚åˆ†æ•£ãŒå°ã•ã„ã»ã©ã€å¹³å‡ã®å¤‰åŒ–ã«æ•æ„Ÿã€‚
- **åˆ†æ•£ $\sigma^2$ ã®æ„Ÿåº¦**ï¼š $2/\sigma^4$ ã«æ¯”ä¾‹ã€‚åˆ†æ•£ãŒå°ã•ã„ã»ã©ã€ã•ã‚‰ã«æ•æ„Ÿã€‚

ã¤ã¾ã‚Šã€ã€Œé‹­ã„åˆ†å¸ƒï¼ˆå°ã•ã„ $\sigma$ ï¼‰ã€ã®æ–¹ãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰åŒ–ã«å¯¾ã—ã¦æ•æ„Ÿã§ã‚ã‚‹ã€‚

> [!TIP]
> **ç‰©ç†çš„ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**ï¼šãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±ã¯ã€ç‰©ç†å­¦ã«ãŠã‘ã‚‹ã€Œæœ‰åŠ¹è³ªé‡ã€ã«ä¼¼ã¦ã„ã‚‹ã€‚é‡ã„ç‰©ä½“ã‚’å‹•ã‹ã™ã«ã¯å¤§ããªåŠ›ãŒå¿…è¦ã ãŒã€è»½ã„ç‰©ä½“ã¯å°ã•ãªåŠ›ã§å‹•ãã€‚æƒ…å ±å¯†åº¦ã®é«˜ã„å ´æ‰€ã§ã¯ã€åˆ†å¸ƒã‚’å‹•ã‹ã™ã®ã«ã€Œå¤§ããªå‹¾é…ã€ãŒå¿…è¦ã«ãªã‚‹ã€‚

## è‡ªç„¶å‹¾é…æ³•ï¼šæ›²ãŒã£ãŸç©ºé–“ã®æ­©ãæ–¹

### é€šå¸¸ã®å‹¾é…æ³•ã®å•é¡Œï¼šç©ºé–“ã®æ­ªã¿ã‚’ç„¡è¦–

é€šå¸¸ã®å‹¾é…é™ä¸‹æ³•ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãŒ **å¹³å¦ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ï¼‰** ã ã¨ä»®å®šã—ã¦é€²ã‚€ï¼š

$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}$$

ã—ã‹ã—ã€å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã¯æ›²ãŒã£ã¦ã„ã‚‹ã€‚ç‰¹ã«ã€æƒ…å ±å¯†åº¦ã®é«˜ã„å ´æ‰€ï¼ˆã‚¸ãƒ£ãƒ³ã‚°ãƒ«ï¼‰ã§ã¯ã€åŒã˜å¤§ãã•ã®å‹¾é…ã§ã‚‚ã€å®Ÿéš›ã®åˆ†å¸ƒã®å¤‰åŒ–é‡ãŒç•°ãªã‚‹ã€‚

ã“ã‚Œã¯ã€åœ°å›³ï¼ˆå¹³é¢ï¼‰ã¨å®Ÿéš›ã®åœ°å½¢ï¼ˆæ›²é¢ï¼‰ã‚’æ··åŒã™ã‚‹ã‚ˆã†ãªã‚‚ã®ã ã€‚åœ°å›³ä¸Šã§1cmã®ç§»å‹•ãŒã€å¹³åœ°ã§ã¯100mã«å¯¾å¿œã™ã‚‹ãŒã€å±±å²³åœ°å¸¯ã§ã¯50mã«ã—ã‹å¯¾å¿œã—ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚

### è‡ªç„¶å‹¾é…ï¼šç©ºé–“ã®æ­ªã¿ã‚’è£œæ­£ã™ã‚‹

**è‡ªç„¶å‹¾é…æ³•ï¼ˆNatural Gradient Descentï¼‰** ã¯ã€ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã‚’ä½¿ã£ã¦å‹¾é…ã‚’è£œæ­£ã™ã‚‹ï¼š

$$\tilde{\nabla} _\theta \mathcal{L} = G(\theta)^{-1} \nabla _\theta \mathcal{L}$$

$$\theta_{t+1} = \theta_t - \eta \tilde{\nabla}_\theta \mathcal{L}$$

ã“ã®è£œæ­£ã«ã‚ˆã‚Šã€**åˆ†å¸ƒç©ºé–“ã§ã®ã€Œç­‰ã—ã„å¤‰åŒ–é‡ã€ã‚’å®Ÿç¾ã™ã‚‹**æœ€çŸ­çµŒè·¯ã‚’é€²ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚

| æ‰‹æ³• | ä½¿ã†å‹¾é… | ç©ºé–“ã®æ‰±ã„ | åˆ©ç‚¹ | æ¬ ç‚¹ |
| --- | --- | --- | --- | --- |
| **é€šå¸¸å‹¾é…** | $\nabla_\theta \mathcal{L}$ | å¹³å¦ã¨ä»®å®š | è¨ˆç®—ãŒè»½ã„ | å¯†åº¦ã®é«˜ã„å ´æ‰€ã§åœæ» |
| **è‡ªç„¶å‹¾é…** | $G^{-1} \nabla_\theta \mathcal{L}$ | æ›²ãŒã‚Šã‚’è€ƒæ…® | åæŸãŒé€Ÿã„ | $G^{-1}$ ã®è¨ˆç®—ãŒé‡ã„ |

### å®Ÿè£…ä¸Šã®èª²é¡Œã¨è¿‘ä¼¼

ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã®é€†è¡Œåˆ— $G^{-1}$ ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯è¨ˆç®—é‡çš„ã«å›°é›£ã§ã‚ã‚‹ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ $n$ ã®ã¨ãã€ $O(n^2)$ ã®ãƒ¡ãƒ¢ãƒªã¨ $O(n^3)$ ã®è¨ˆç®—ï¼‰ã€‚

ãã®ãŸã‚ã€å®Ÿç”¨çš„ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªè¿‘ä¼¼æ‰‹æ³•ãŒä½¿ã‚ã‚Œã‚‹ï¼š

- **K-FAC (Kronecker-Factored Approximate Curvature)**ï¼šãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã‚’Kroneckerç©ã§è¿‘ä¼¼
- **å¯¾è§’è¿‘ä¼¼**ï¼šéå¯¾è§’è¦ç´ ã‚’ç„¡è¦–ã—ã€å¯¾è§’æˆåˆ†ã®ã¿ã‚’ä½¿ã†ç°¡æ˜“ç‰ˆ
- **Adamç­‰ã®é©å¿œçš„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶**ï¼šå‹¾é…ã®2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’åº§æ¨™ã”ã¨ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€Œå‰å‡¦ç†ã€ã¨ã—ã¦ã€ç‰¹å®šæ¡ä»¶ä¸‹ã§è‡ªç„¶å‹¾é…çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã™ã‚‹ã“ã¨ãŒã‚ã‚‹

> [!NOTE]
> **Adamã¨è‡ªç„¶å‹¾é…ã®é–¢ä¿‚**ï¼šAdamã¯å‹¾é…ã‚’åº§æ¨™ã”ã¨ã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹å‰å‡¦ç†ã§ã‚ã‚Šã€ã“ã‚ŒãŒç‰¹å®šã®æ¡ä»¶ï¼ˆå¯¾è§’Fisherè¿‘ä¼¼ã€å®šå¸¸åˆ†å¸ƒãªã©ï¼‰ä¸‹ã§è‡ªç„¶å‹¾é…ã«é¡ä¼¼ã—ãŸåŠ¹æœã‚’æŒã¤å ´åˆãŒã‚ã‚‹ã€‚ã—ã‹ã—ã€AdamãŒã€Œè‡ªç„¶å‹¾é…ã®è¿‘ä¼¼ã€ã¨è¨€ã„åˆ‡ã‚‹ã®ã¯éå‰°ä¸»å¼µã§ã‚ã‚‹ã€‚ä¸¡è€…ã¯è¨­è¨ˆæ€æƒ³ãŒç•°ãªã‚Šã€Adam ã¯ Fisher è¨ˆé‡ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã—ã¦ã„ãªã„ã€‚

```appendix4_natural_gradient_concept.py
import torch

# ã€é‡è¦ã€‘ä»¥ä¸‹ã¯ã€Œè‡ªç„¶å‹¾é…æ³•ã®å½¢å¼ã€ã‚’ç¤ºã™æ•™è‚²çš„ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã§ã‚ã‚Šã€
# Fisheræƒ…å ±è¡Œåˆ—ã‚’å˜ä½è¡Œåˆ—ã§ä»£ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€è‡ªç„¶å‹¾é…ã®æ€§è³ªã‚’ä½•ã‚‚ç¤ºã•ãªã„ã€‚
# å®Ÿéš›ã«ã¯å˜ãªã‚‹å­¦ç¿’ç‡ã®å†ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ(1+damping)å€ï¼‰ã«éããªã„ã€‚


def natural_gradient_step(model, loss, lr=0.01, damping=1e-4):
    """è‡ªç„¶å‹¾é…æ³•ã®"å½¢å¼"ã‚’ç¤ºã™ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼ˆå®Ÿç”¨ä¸å¯ï¼‰

    è­¦å‘Š: æœ¬å®Ÿè£…ã¯Fisheræƒ…å ±ã‚’å˜ä½è¡Œåˆ—ã§ä»£ç”¨ã—ã¦ãŠã‚Šã€
    è‡ªç„¶å‹¾é…ã®æœ¬è³ªï¼ˆãƒ¢ãƒ‡ãƒ«ä¾å­˜ã®è¨ˆé‡ï¼‰ãŒå®Œå…¨ã«å¤±ã‚ã‚Œã¦ã„ã‚‹ã€‚
    fisher_approx = (1+damping)*I ãªã®ã§ã€ã“ã‚Œã¯å˜ãªã‚‹ã‚¹ã‚«ãƒ©ãƒ¼å€ã§ã‚ã‚Šã€
    è‡ªç„¶å‹¾é…ãŒæä¾›ã™ã‚‹ã€Œç©ºé–“ã®æ­ªã¿ã®è£œæ­£ã€ã¯ä¸€åˆ‡è¡Œã‚ã‚Œãªã„ã€‚

    çœŸã®å®Ÿè£…ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›åˆ†å¸ƒã‹ã‚‰Fisherã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    """
    # é€šå¸¸ã®å‹¾é…
    loss.backward()

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’1æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«
    params = torch.cat([p.flatten() for p in model.parameters()])
    grads = torch.cat([p.grad.flatten() for p in model.parameters()])

    # ã€å•é¡Œç‚¹ã€‘Fisherã‚’å˜ä½è¡Œåˆ—ã§ä»£ç”¨ â†’ ã“ã‚Œã§ã¯è‡ªç„¶å‹¾é…ã«ãªã‚‰ãªã„
    # fisher_approx = I + damping*I = (1+damping)*I
    # ã¤ã¾ã‚Šã€æ’ç­‰å¤‰æ›ã®ã‚¹ã‚«ãƒ©ãƒ¼å€ = å­¦ç¿’ç‡ã‚’ (1+damping) å€ã™ã‚‹ã ã‘
    # å®Ÿéš›ã«ã¯ã€å‡ºåŠ›ã®log probabilityã®å‹¾é…ã®å¤–ç©ã®æœŸå¾…å€¤ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
    fisher_approx = torch.eye(len(params)) + damping * torch.eye(len(params))

    # "è‡ªç„¶å‹¾é…ã®ã‚ˆã†ãªå½¢" = Fisher^{-1} @ grad
    # ï¼ˆãŸã ã—Fisher = (1+damping)*I ãªã®ã§ã€å®Ÿè³ª grad / (1+damping)ï¼‰
    natural_grad = torch.linalg.solve(fisher_approx, grads)

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
    idx = 0
    for p in model.parameters():
        p_len = p.numel()
        p.data -= lr * natural_grad[idx : idx + p_len].view_as(p)
        idx += p_len
        p.grad.zero_()


# ã€æ¨å¥¨ã€‘å®Ÿç”¨ã«ã¯K-FACç­‰ã®å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã¾ãŸã¯å¯¾è§’Fisherè¿‘ä¼¼ã‚’ç”¨ã„ã‚‹ã¹ã
```

> [!CAUTION]
> ä¸Šè¨˜ã¯**è‡ªç„¶å‹¾é…æ³•ãã®ã‚‚ã®ã§ã¯ãªãã€ãã®"å½¢å¼"ã‚’ç¤ºã™æ•™è‚²çš„ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰**ã§ã‚ã‚‹ã€‚Fisheræƒ…å ±è¡Œåˆ—ã‚’ $(1+\text{damping}) \cdot I$ ã§ä»£ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã‚Œã¯å˜ãªã‚‹å­¦ç¿’ç‡ã®å†ã‚¹ã‚±ãƒ¼ãƒ«ã«ç­‰ã—ãã€è‡ªç„¶å‹¾é…ãŒæä¾›ã™ã‚‹ã€Œç©ºé–“ã®æ­ªã¿ã«å¿œã˜ãŸæ–¹å‘è£œæ­£ã€ã¯ä¸€åˆ‡è¡Œã‚ã‚Œãªã„ã€‚çœŸã®è‡ªç„¶å‹¾é…æ³•ã‚’å®Ÿè£…ã™ã‚‹ã«ã¯ã€K-FACï¼ˆMartens & Grosse, 2015ï¼‰ãªã©ã®å°‚ç”¨æ‰‹æ³•ã€ã¾ãŸã¯å°‘ãªãã¨ã‚‚å¯¾è§’Fisherè¿‘ä¼¼ï¼ˆãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®åˆ†å¸ƒã‹ã‚‰å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Fisherå¯¾è§’æˆåˆ†ã‚’è¨ˆç®—ï¼‰ãŒå¿…è¦ã§ã‚ã‚‹ã€‚

> [!NOTE]
> **ç¬¬4å›ã¨ã®æ¥ç¶š**ï¼šç¬¬4å›ã§è‡ªç„¶å‹¾é…æ³•ã«è§¦ã‚ŒãŸéš›ã€ãã‚ŒãŒSoftmaxã‚„ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å¹¾ä½•å­¦çš„æ„å‘³ã¨çµã³ã¤ã„ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ãŸã€‚æœ¬Appendixã§ã¯ã€ãã®ç†è«–çš„èƒŒæ™¯ã§ã‚ã‚‹ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã‚’æ˜ç¤ºçš„ã«å°å…¥ã—ã€è‡ªç„¶å‹¾é…æ³•ã‚’ **ç©ºé–“ã®æ›²ãŒã‚Šã‚’è£œæ­£ã™ã‚‹æœ€é©åŒ–æ‰‹æ³•** ã¨ã—ã¦ä½ç½®ã¥ã‘ãŸã€‚

## çµè«–ï¼šAttentionã¨MoEã¸ã®æ¥ç¶š

### æƒ…å ±å¯†åº¦ã®åœ°å›³ã‚’æ¸¬é‡ã™ã‚‹AIï¼ˆæ¯”å–©çš„ç†è§£ï¼‰

ã“ã‚Œã¾ã§è¦‹ã¦ããŸã€Œç‰©å·®ã—ã€ã®æ¦‚å¿µã‚’ã€å…·ä½“çš„ãªãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã¨**æ¯”å–©çš„ã«å¯¾å¿œã¥ã‘ã¦**ç†è§£ã—ã¦ã¿ã‚ˆã†ã€‚

**AttentionãŒç‰¹å®šã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é‡ã¿ã‚’ç½®ãã“ã¨**ã€**MoEãŒç‰¹å®šã®Expertã‚’é¸ã¶ã“ã¨** ã¯ã€ãã®é ˜åŸŸãŒ **ã€Œé‡è¦ã€ã¾ãŸã¯ã€Œæ„Ÿåº¦ãŒé«˜ã„ã€** ã¨åˆ¤æ–­ã—ã¦ã„ã‚‹ã“ã¨ã¨ã€ç›´æ„Ÿçš„ã«å¯¾å¿œã¥ã‘ã‚‰ã‚Œã‚‹ã€‚

- **Attention**ï¼šQueryã¨Keyã®å†…ç©ï¼ˆscaled dot-productï¼‰ãŒé«˜ã„ = ãã®æ–¹å‘ãŒã€Œé‡è¦ã€
- **MoE**ï¼šï¼ˆå…¸å‹çš„ã«ã¯ï¼‰å…¥åŠ›ç‰¹å¾´ã«å¯¾ã™ã‚‹ç·šå½¢ã‚¹ã‚³ã‚¢ï¼ˆå†…ç©ã«ç›¸å½“ï¼‰ãŒé«˜ã„Expertã‚’é¸æŠ = ãã®éƒ¨åˆ†ç©ºé–“ãŒã€Œé‡è¦ã€

ãŸã ã—ã€**å³å¯†ã«ã¯**ï¼š

- **Attentionã¯å†…ç©å¹¾ä½•ã«åŸºã¥ãé¡ä¼¼åº¦è¨ˆç®—**ï¼ˆè¡¨ç¾ç©ºé–“ä¸Šã®æ“ä½œï¼‰
- **Fisheræƒ…å ±ã¯çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã®è¨ˆé‡**ï¼ˆç¢ºç‡åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ä¸Šã®æ“ä½œï¼‰
- **å¯¾è±¡ç©ºé–“ãŒç•°ãªã‚‹**ï¼šã€Œé‡è¦åº¦ã€ã¯è¡¨ç¾ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰ä¸Šã®æ¦‚å¿µã€Fisherã®ã€Œæ„Ÿåº¦ã€ã¯ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ï¼ˆå‡ºåŠ›åˆ†å¸ƒï¼‰ä¸Šã®æ¦‚å¿µ

ã“ã“ã§ã®ã€Œæƒ…å ±å¯†åº¦ã€ã¯ã€æ•™è‚²çš„ãªæ¯”å–©ã¨ã—ã¦æ‰ãˆã‚‹ã¹ãã§ã‚ã‚‹ã€‚

ã©ã¡ã‚‰ã‚‚ã€**é«˜æ¬¡å…ƒç©ºé–“ã®ã©ã®æ–¹å‘ãƒ»éƒ¨åˆ†ç©ºé–“ã«è¨ˆç®—è³‡æºã‚’å‰²ã‚Šå½“ã¦ã‚‹ã‹**ã‚’ã€å‹•çš„ã«æ±ºå®šã—ã¦ã„ã‚‹ç‚¹ã§å…±é€šã—ã¦ã„ã‚‹ã€‚

### å­¦ç¿’ã«ã‚ˆã‚‹æƒ…å ±ã®åœ°å½¢ã®å½¢æˆï¼ˆåŒæ–¹å‘çš„ãªé–¢ä¿‚ï¼‰

ã•ã‚‰ã«é‡è¦ãªã®ã¯ã€ã“ã®ã€Œæƒ…å ±ã®åœ°å½¢ã€ã¯ **å­¦ç¿’ã«ã‚ˆã£ã¦å½¢æˆã•ã‚Œã‚‹** ã¨ã„ã†ç‚¹ã§ã‚ã‚‹ã€‚å› æœé–¢ä¿‚ã¯ä¸€æ–¹å‘ã§ã¯ãªãã€ç›¸äº’ä½œç”¨çš„ã§ã‚ã‚‹ã€‚

åˆæœŸåŒ–ç›´å¾Œã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æƒ…å ±å¯†åº¦ï¼ˆFisheræƒ…å ±ï¼‰ãŒã»ã¼ä¸€æ§˜ãªã€Œå¹³ã‚‰ãªç ‚æ¼ ã€ã®ã‚ˆã†ãªç©ºé–“ã‹ã‚‚ã—ã‚Œãªã„ã€‚ã—ã‹ã—ã€å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã¦ï¼š

1. **å‹¾é…ï¼ˆKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼‰** ãŒã€ãƒ¢ãƒ‡ãƒ«ã‚’çœŸã®åˆ†å¸ƒã«å¼•ãå¯„ã›ã‚‹
2. **ç‰¹å®šã®æ–¹å‘ãƒ»é ˜åŸŸã®é‡è¦æ€§ãŒé«˜ã¾ã‚‹**ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚„æå¤±é–¢æ•°ã«å¿œã˜ã¦ï¼‰
3. **çµæœã¨ã—ã¦ã€Fisheræƒ…å ±ãŒå¤§ãããªã‚‹æ–¹å‘ãŒç¾ã‚Œã‚‹**ï¼ˆã‚¸ãƒ£ãƒ³ã‚°ãƒ«ãŒå½¢æˆã•ã‚Œã‚‹ï¼‰
4. **Attention/MoEã¯ã€å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ç©ºé–“ã§ã€Œé¡ä¼¼åº¦ãŒé«˜ã„ã€æ–¹å‘ã«é‡ã¿ã‚’ç½®ã**

ã¤ã¾ã‚Šã€ã€ŒAttentionãŒæƒ…å ±å¯†åº¦ã®é«˜ã„å ´æ‰€ã«é‡ã¿ã‚’ç½®ãã€ã¨ã€Œå­¦ç¿’ã«ã‚ˆã£ã¦ç‰¹å®šæ–¹å‘ã®æ„Ÿåº¦ãŒé«˜ã¾ã‚‹ã€ã¯ã€**ç›¸äº’ã«å½±éŸ¿ã—åˆã†åŒæ–¹å‘ã®é–¢ä¿‚**ã«ã‚ã‚‹ã€‚ä¸€æ–¹ãŒä»–æ–¹ã‚’æ±ºå®šã™ã‚‹ã®ã§ã¯ãªãã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§å…±é€²åŒ–ã™ã‚‹ã€‚

```txt
å­¦ç¿’ã«ã‚ˆã‚‹æƒ…å ±åœ°å½¢ã®å¤‰åŒ–ï¼ˆæ¦‚å¿µå›³ï¼‰:

åˆæœŸçŠ¶æ…‹ï¼ˆå¹³å¦ï¼‰:
ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸœï¸ ğŸœï¸
ã™ã¹ã¦ã®æ–¹å‘ãŒç­‰ä¾¡

     â†“ å­¦ç¿’

å­¦ç¿’å¾Œï¼ˆèµ·ä¼ã‚ã‚Šï¼‰:
â›°ï¸  â›°ï¸  ğŸœï¸ â›°ï¸  ğŸœï¸ ğŸœï¸
é‡è¦ãªæ–¹å‘ã«æƒ…å ±å¯†åº¦ãŒé›†ä¸­
```

### 3ã¤ã®ç‰©å·®ã—ã®çµ±åˆ

æœ¬Appendixã§æ‰±ã£ãŸ3ã¤ã®ã€Œç‰©å·®ã—ã€ã‚’çµ±åˆã™ã‚‹ã¨ï¼š

| ç‰©å·®ã— | å¯¾ç§°æ€§ | æ¸¬ã‚‹ã‚‚ã® | AIãƒ¢ãƒ‡ãƒ«ã§ã®å½¹å‰² | å‚™è€ƒ |
| --- | --- | --- | --- | --- |
| **å†…ç©** | å¯¾ç§° | ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã¨æ–¹å‘ | Attentionï¼ˆscaled dot-productï¼‰ | å†…ç©å¹¾ä½•ã«åŸºã¥ã |
| **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦** | å¯¾ç§° | ãƒ™ã‚¯ãƒˆãƒ«ã®æ–¹å‘ï¼ˆæ­£è¦åŒ–å¾Œï¼‰ | æ­£è¦åŒ–åŸ‹ã‚è¾¼ã¿ã€æ­£è¦åŒ–Attention | L2æ­£è¦åŒ–å¾Œã®å†…ç© |
| **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** | éå¯¾ç§° | åˆ†å¸ƒã®éš”ãŸã‚Šã¨æ–¹å‘ | å­¦ç¿’ã®é§†å‹•åŠ›ï¼ˆæå¤±é–¢æ•°ï¼‰ | çµ±è¨ˆçš„è·é›¢ï¼ˆéå¯¾ç§°ï¼‰ |
| **ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±** | è¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ« | ç©ºé–“ã®æ„Ÿåº¦ãƒ»å¯†åº¦ | æœ€é©åŒ–ã®åŠ¹ç‡åŒ–ï¼ˆè‡ªç„¶å‹¾é…ï¼‰ | Attention/MoEã¨ã¯**æ¯”å–©çš„å¯¾å¿œ** |

ã“ã‚Œã‚‰ã¯ç‹¬ç«‹ã—ãŸæ¦‚å¿µã§ã¯ãªãã€**æƒ…å ±å¹¾ä½•å­¦**ã¨ã„ã†çµ±ä¸€çš„ãªæ çµ„ã¿ã®ä¸­ã§ã€ç•°ãªã‚‹å´é¢ã‚’ç…§ã‚‰ã—å‡ºã—ã¦ã„ã‚‹ã€‚

> [!IMPORTANT]
> **æ¯”å–©ã¨å³å¯†æ€§ã®åŒºåˆ¥**ï¼šæœ¬Appendixã§ã¯ã€ŒAttentionã‚„MoEãŒæƒ…å ±å¯†åº¦ã®é«˜ã„å ´æ‰€ã«è¨ˆç®—è³‡æºã‚’æŠ•ã˜ã‚‹ã€ã¨ã„ã†è¡¨ç¾ã‚’ç”¨ã„ãŸãŒã€ã“ã‚Œã¯**ç›´æ„Ÿçš„ç†è§£ã‚’åŠ©ã‘ã‚‹ãŸã‚ã®æ¯”å–©çš„å¯¾å¿œã¥ã‘**ã§ã‚ã‚‹ã€‚å³å¯†ã«ã¯ã€Attentionã¯æ¨™æº–çš„ã«ã¯Query-Keyã®å†…ç©ï¼ˆscaled dot-productï¼‰ã€MoEã¯ï¼ˆå…¸å‹çš„ã«ã¯ï¼‰å…¥åŠ›ã¨ã‚²ãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©ã«åŸºã¥ã„ã¦ãŠã‚Šã€Fisheræƒ…å ±ï¼ˆçµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è¨ˆé‡ï¼‰ã¨ã¯åˆ¥ç‰©ã§ã‚ã‚‹ã€‚ä¸¡è€…ã®é–¢ä¿‚ã¯ã€ã€Œé‡è¦åº¦ãƒ»æ„Ÿåº¦ãŒé«˜ã„æ–¹å‘ã¸ã®è³‡æºé…åˆ†ã€ã¨ã„ã†æŠ½è±¡ãƒ¬ãƒ™ãƒ«ã§ã®é¡ä¼¼æ€§ã§ã‚ã‚Šã€æ•°å­¦çš„åŒä¸€æ€§ã§ã¯ãªã„ã€‚

## å®Ÿè£…ãƒãƒ¼ãƒˆï¼šæƒ…å ±ã®åœ°å½¢ã‚’å¯è¦–åŒ–ã™ã‚‹

### KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éå¯¾ç§°æ€§ã®ç¢ºèª

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹: appendix4_kl_asymmetry_viz.py</summary>

```appendix4_kl_asymmetry_viz.py
import matplotlib.pyplot as plt
import numpy as np
import torch


def kl_divergence(p, q):
    """KL(P||Q) ã‚’è¨ˆç®—ï¼ˆ0é™¤ç®—å›é¿ä»˜ãï¼‰"""
    epsilon = 1e-10
    return (p * torch.log((p + epsilon) / (q + epsilon))).sum()


# 2ã¤ã®åˆ†å¸ƒã‚’å®šç¾©
P = torch.tensor([0.7, 0.2, 0.1])
Q_list = []
kl_pq_list = []
kl_qp_list = []

# Qã®æœ€åˆã®è¦ç´ ã‚’å¤‰åŒ–ã•ã›ã‚‹
for q0 in np.linspace(0.1, 0.9, 50):
    Q = torch.tensor([q0, (1 - q0) * 0.6, (1 - q0) * 0.4])
    Q_list.append(q0)
    kl_pq_list.append(kl_divergence(P, Q).item())
    kl_qp_list.append(kl_divergence(Q, P).item())

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(Q_list, kl_pq_list, label="KL(P||Q)", linewidth=2)
plt.plot(Q_list, kl_qp_list, label="KL(Q||P)", linewidth=2, linestyle="--")
plt.xlabel("Q[0]")
plt.ylabel("KL Divergence")
plt.title("KL Divergence Asymmetry")
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig("kl_asymmetry.png", dpi=150, bbox_inches="tight")
print("KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éå¯¾ç§°æ€§ã‚’ kl_asymmetry.png ã«ä¿å­˜")
```

</details>

### ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã®æ•°å€¤è¨ˆç®—ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹: appendix4_fisher_gaussian.py</summary>

```appendix4_fisher_gaussian.py
import torch


def fisher_information_gaussian(mu, sigma):
    """æ­£è¦åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ï¼ˆè§£æçš„ï¼‰

    N(mu, sigma^2) ã®ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—:
    [[1/sigma^2, 0],
     [0, 2/sigma^4]]
    """
    return torch.tensor([[1 / sigma**2, 0], [0, 2 / sigma**4]])


# ç•°ãªã‚‹åˆ†æ•£ã§ã®æ¯”è¼ƒ
sigmas = [0.5, 1.0, 2.0]

for sigma in sigmas:
    fisher = fisher_information_gaussian(mu=0.0, sigma=sigma)
    print(f"Ïƒ={sigma}:")
    print(f"  F_Î¼Î¼ = {fisher[0, 0]:.4f} (å¹³å‡ã¸ã®æ„Ÿåº¦)")
    print(f"  F_ÏƒÏƒ = {fisher[1, 1]:.4f} (åˆ†æ•£ã¸ã®æ„Ÿåº¦)")
    print()

# å‡ºåŠ›ä¾‹:
# Ïƒ=0.5:
#   F_Î¼Î¼ = 4.0000 (å¹³å‡ã¸ã®æ„Ÿåº¦)  â† åˆ†æ•£ãŒå°ã•ã„ã»ã©æ„Ÿåº¦ãŒé«˜ã„
#   F_ÏƒÏƒ = 32.0000 (åˆ†æ•£ã¸ã®æ„Ÿåº¦)
#
# Ïƒ=1.0:
#   F_Î¼Î¼ = 1.0000
#   F_ÏƒÏƒ = 2.0000
#
# Ïƒ=2.0:
#   F_Î¼Î¼ = 0.2500  â† åˆ†æ•£ãŒå¤§ãã„ã¨æ„Ÿåº¦ãŒä½ã„
#   F_ÏƒÏƒ = 0.1250
```

</details>

### è‡ªç„¶å‹¾é…ã¨é€šå¸¸å‹¾é…ã®æ¯”è¼ƒï¼ˆç©å…·å•é¡Œï¼‰

<details>
<summary>ã‚³ãƒ¼ãƒ‰ä¾‹: appendix4_natural_vs_standard.py</summary>

```appendix4_natural_vs_standard.py
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F


# ç°¡å˜ãª2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
class SimpleLogistic(nn.Module):
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.tensor([0.0, 0.0]))

    def forward(self, x):
        return torch.sigmoid(x @ self.w)


# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç·šå½¢åˆ†é›¢å¯èƒ½ï¼‰
torch.manual_seed(42)
X = torch.randn(100, 2)
y = (X[:, 0] + X[:, 1] > 0).float()

# é€šå¸¸ã®å‹¾é…é™ä¸‹
model_sgd = SimpleLogistic()
optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.1)
losses_sgd = []

for _ in range(100):
    optimizer_sgd.zero_grad()
    pred = model_sgd(X)
    loss = F.binary_cross_entropy(pred, y)
    loss.backward()
    optimizer_sgd.step()
    losses_sgd.append(loss.item())

# é©å¿œçš„å‰å‡¦ç†ï¼ˆAdamã§è¿‘ä¼¼çš„ã«ï¼‰
model_adam = SimpleLogistic()
optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.1)
losses_adam = []

for _ in range(100):
    optimizer_adam.zero_grad()
    pred = model_adam(X)
    loss = F.binary_cross_entropy(pred, y)
    loss.backward()
    optimizer_adam.step()
    losses_adam.append(loss.item())

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(losses_sgd, label="SGD (Standard Gradient)", linewidth=2)
plt.plot(losses_adam, label="Adam (Adaptive Preconditioning)", linewidth=2)
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Convergence: Standard Gradient vs Adaptive Preconditioning")
plt.legend()
plt.grid(True, alpha=0.3)
plt.yscale("log")
plt.savefig("gradient_comparison.png", dpi=150, bbox_inches="tight")
print("åæŸã®æ¯”è¼ƒã‚’ gradient_comparison.png ã«ä¿å­˜")
```

</details>

> [!NOTE]
> ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€Adamã‚’ **é©å¿œçš„å‰å‡¦ç†ï¼ˆåº§æ¨™ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰** ã¨ã—ã¦æ‰±ã£ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ç‰¹å®šæ¡ä»¶ä¸‹ã§è‡ªç„¶å‹¾é…çš„ãªåŠ¹æœã‚’æŒã¤ã“ã¨ãŒã‚ã‚‹ãŒã€å³å¯†ã«ã¯è‡ªç„¶å‹¾é…æ³•ï¼ˆFisherè¨ˆé‡ã«åŸºã¥ãæœ€é©åŒ–ï¼‰ã¨ã¯ç•°ãªã‚‹ã€‚çœŸã®è‡ªç„¶å‹¾é…æ³•ã‚’å®Ÿè£…ã™ã‚‹ã«ã¯ã€K-FACãªã©ã®å°‚ç”¨æ‰‹æ³•ãŒå¿…è¦ã€‚

## ã¾ã¨ã‚

| æ¦‚å¿µ | å®šç¾© | æ·±å±¤å­¦ç¿’ã§ã®å½¹å‰² |
| --- | --- | --- |
| **å†…ç©ï¼ˆdot productï¼‰** | ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã¨æ–¹å‘ã®ç© | Attentionï¼ˆscaled dot-productï¼‰ã®åŸºç¤ |
| **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦** | æ­£è¦åŒ–å¾Œã®å†…ç©ï¼ˆæ–¹å‘ã®ã¿ï¼‰ | æ­£è¦åŒ–åŸ‹ã‚è¾¼ã¿ã€æ­£è¦åŒ–Attentionï¼ˆnGPTãªã©ï¼‰ |
| **åŒæ›²è·é›¢** | è² æ›²ç‡ç©ºé–“ã§ã®è·é›¢ | éšå±¤æ§‹é€ ã®è¡¨ç¾ï¼ˆç¬¬12å›ï¼‰ |
| **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** | åˆ†å¸ƒã®éå¯¾ç§°ãªéš”ãŸã‚Š | æå¤±é–¢æ•°ã€å­¦ç¿’ã®é§†å‹• |
| **ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—** | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è¨ˆé‡ | è‡ªç„¶å‹¾é…æ³•ã€ç©ºé–“ã®æ„Ÿåº¦ |
| **è‡ªç„¶å‹¾é…æ³•** | æ›²ãŒã£ãŸç©ºé–“ã§ã®æœ€é©åŒ– | åŠ¹ç‡çš„ãªå­¦ç¿’ï¼ˆç†è«–ä¸Šï¼‰ |

### ã‚´ãƒ¼ãƒ«

**ã€Œæ¸¬ã‚‹ã€ã“ã¨ã¯ã€Œç†è§£ã™ã‚‹ã€ã“ã¨ã®ç¬¬ä¸€æ­©ã§ã‚ã‚‹ã€‚**

æœ¬Appendixã‚’é€šã˜ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå•ã„ã‚’è‡ªç„¶ã«ç™ºã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç›´æ„Ÿã‚’ç²å¾—ã—ã¦ã»ã—ã„ï¼š

- ã€Œã“ã®ç©ºé–“ã§ã¯ã€ä½•ã‚’ã€è¿‘ã„ã€ã¨å®šç¾©ã—ã¦ã„ã‚‹ã®ã‹ã€
- ã€Œã“ã®æå¤±é–¢æ•°ã¯ã€ã©ã¡ã‚‰ã‹ã‚‰ã©ã¡ã‚‰ã¸ã®ã€éš”ãŸã‚Šã€ã‚’æ¸¬ã£ã¦ã„ã‚‹ã®ã‹ã€
- ã€Œã“ã®æœ€é©åŒ–æ‰‹æ³•ã¯ã€ç©ºé–“ã®ã©ã‚“ãªæ€§è³ªã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã®ã‹ã€
- ã€ŒAIãƒ¢ãƒ‡ãƒ«ã¯ã€æƒ…å ±ã®åœ°å½¢ã®ã©ã“ã«è¨ˆç®—è³‡æºã‚’æŠ•ã˜ã¦ã„ã‚‹ã®ã‹ã€

### è¬›ç¾©æœ¬ç·¨ã¨ã®æ¥ç¶š

æœ¬Appendixã§æ‰±ã£ãŸæ¦‚å¿µã¯ã€ä»¥ä¸‹ã®å›ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ã„ã‚‹ï¼š

- **ç¬¬4å›ï¼ˆSoftmax/æƒ…å ±å¹¾ä½•ï¼‰**ï¼šKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€è‡ªç„¶å‹¾é…æ³•ã®å¿œç”¨
- **ç¬¬6å›ï¼ˆAttentionï¼‰**ï¼šå†…ç©ï¼ˆscaled dot-productï¼‰ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—ã€‚æ¨™æº–çš„ãªAttentionã¯å†…ç©ã‚’ä½¿ç”¨ã—ã€ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã‚‚è€ƒæ…®ã™ã‚‹ã€‚æ­£è¦åŒ–Attentionã§ã¯ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ç›¸å½“ã€‚
- **ç¬¬12å›ï¼ˆåŒæ›²å¹¾ä½•ï¼‰**ï¼šåŒæ›²è·é›¢ã«ã‚ˆã‚‹éšå±¤æ§‹é€ ã®è¡¨ç¾
- **ç¬¬13å›ï¼ˆé«˜æ¬¡å…ƒ/MoEï¼‰**ï¼šè¨ˆç®—è³‡æºã®å‹•çš„é…åˆ†ï¼ˆFisheræƒ…å ±ã¨ã®**æ¯”å–©çš„å¯¾å¿œ**ã¨ã—ã¦ç†è§£å¯èƒ½ï¼‰

ã“ã‚Œã‚‰ã®å›ã‚’èª­ã‚€éš›ã€æœ¬Appendixã§å¾—ãŸã€Œç‰©å·®ã—ã€ã®è¦–ç‚¹ã‚’æ€ã„å‡ºã™ã“ã¨ã§ã€ã‚ˆã‚Šæ·±ã„ç†è§£ãŒå¾—ã‚‰ã‚Œã‚‹ã ã‚ã†ã€‚ãŸã ã—ã€**Attentionã‚„MoEã®è¨­è¨ˆåŸç†ã¯Fisheræƒ…å ±ãã®ã‚‚ã®ã§ã¯ãªãã€å†…ç©å¹¾ä½•ã‚„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ©Ÿæ§‹ã«åŸºã¥ã„ã¦ã„ã‚‹**ç‚¹ã‚’å¿µé ­ã«ç½®ãã“ã¨ã€‚

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼šå‹•æ…‹è«–ã¸

æœ¬è¬›ç¾©ã€Œçµ±ä¸€è¦–ç‚¹ã€ã¯ã€**ç©ºé–“ã®å½¢**ï¼ˆã©ã“ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰ã‚’æ‰±ã£ãŸã€‚ã—ã‹ã—ã€æƒ…å ±å¹¾ä½•å­¦ã®ã‚‚ã†ä¸€ã¤ã®é¡”ã¯ã€**æƒ…å ±ã®æµã‚Œ**ï¼ˆã©ã†ç§»å‹•ã™ã‚‹ã‹ï¼‰ã§ã‚ã‚‹ã€‚

ç¶šç·¨ã€Œå‹•æ…‹è«–ã€ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã«ãŠã‘ã‚‹ã€Œæƒ…å ±ã®è»Œé“ã€ã€æ¨è«–éç¨‹ã«ãŠã‘ã‚‹ã€Œæ€è€ƒã®é€£é–ã€ãªã©ã€å‹•çš„ãªè¦–ç‚¹ã‚’å°å…¥ã™ã‚‹ã€‚ãã“ã§ã¯ã€æœ¬Appendixã§å­¦ã‚“ã ã€Œç‰©å·®ã—ã€ãŒã€**æ™‚é–“ç™ºå±•ã™ã‚‹ç³»**ã«ãŠã„ã¦ã©ã†æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’è¦‹ã‚‹ã“ã¨ã«ãªã‚‹ã€‚

æƒ…å ±å¹¾ä½•å­¦ã¯ã€é™æ­¢ç”»ã§ã¯ãªãã€å‹•ç”»ã¨ã—ã¦ç†è§£ã•ã‚Œã‚‹ã¹ãã§ã‚ã‚‹ã€‚

## å‚è€ƒæ–‡çŒ®

### æƒ…å ±å¹¾ä½•å­¦ï¼ˆåŸºç¤ï¼‰

- Amari, S. (2016). *Information Geometry and Its Applications*. Applied Mathematical Sciences, Vol. 194. Springer Japan. DOI: [10.1007/978-4-431-55978-8](https://doi.org/10.1007/978-4-431-55978-8)
    - æƒ…å ±å¹¾ä½•å­¦ã®æ¨™æº–çš„æ•™ç§‘æ›¸ã€‚ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±è¡Œåˆ—ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€åŒå¯¾æ§‹é€ ãªã©ã‚’ä½“ç³»çš„ã«æ‰±ã†ã€‚
- Amari, S., & Nagaoka, H. (2000). *Methods of Information Geometry*. Translations of Mathematical Monographs, Vol. 191. American Mathematical Society. DOI: [10.1090/mmono/191](https://doi.org/10.1090/mmono/191)
    - æƒ…å ±å¹¾ä½•å­¦ã®å¤å…¸ã€‚æ•°å­¦çš„ã«å³å¯†ãªå®šå¼åŒ–ã‚’æä¾›ã€‚

### è‡ªç„¶å‹¾é…æ³•

- Amari, S. (1998). Natural Gradient Works Efficiently in Learning. *Neural Computation*, 10(2), 251â€“276. DOI: [10.1162/089976698300017746](https://doi.org/10.1162/089976698300017746)
    - è‡ªç„¶å‹¾é…æ³•ã®ä¸€æ¬¡æ–‡çŒ®ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è¨ˆé‡æ§‹é€ ãŒå­¦ç¿’åŠ¹ç‡ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç¤ºã—ãŸå¤å…¸ã€‚
- Martens, J., & Grosse, R. (2015). Optimizing Neural Networks with Kronecker-factored Approximate Curvature. *ICML 2015*. arXiv: [arXiv:1503.05671](https://arxiv.org/abs/1503.05671)
    - K-FACï¼ˆKronecker-Factored Approximate Curvatureï¼‰ã®ææ¡ˆã€‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®è‡ªç„¶å‹¾é…æ³•ã®è¿‘ä¼¼æ‰‹æ³•ã€‚

### KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨æå¤±é–¢æ•°

- Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. *The Annals of Mathematical Statistics*, 22(1), 79â€“86. DOI: [10.1214/aoms/1177729694](https://doi.org/10.1214/aoms/1177729694)
    - KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ä¸€æ¬¡æ–‡çŒ®ã€‚æƒ…å ±ç†è«–ã«ãŠã‘ã‚‹åŸºç¤æ¦‚å¿µã€‚
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. ISBN: 978-0387310732.
    - æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€æœ€å°¤æ¨å®šã®é–¢ä¿‚ã‚’è©³ã—ãè§£èª¬ã€‚

### åŒæ›²å¹¾ä½•å­¦ï¼ˆéšå±¤æ§‹é€ ï¼‰

- Nickel, M., & Kiela, D. (2017). PoincarÃ© Embeddings for Learning Hierarchical Representations. *NeurIPS 2017*. arXiv: [arXiv:1705.08039](https://arxiv.org/abs/1705.08039)
    - åŒæ›²ç©ºé–“ï¼ˆPoincarÃ©çƒãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã®åŸ‹ã‚è¾¼ã¿æ‰‹æ³•ã€‚éšå±¤æ§‹é€ ã®è¡¨ç¾ã«é©ã—ã¦ã„ã‚‹ã“ã¨ã‚’å®Ÿé¨“çš„ã«ç¤ºã—ãŸã€‚
- Ganea, O., BÃ©cigneul, G., & Hofmann, T. (2018). Hyperbolic Neural Networks. *NeurIPS 2018*. arXiv: [arXiv:1805.09112](https://arxiv.org/abs/1805.09112)
    - åŒæ›²ç©ºé–“ã§ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸€èˆ¬åŒ–ã€‚

### Attention ã¨å†…ç©

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*. arXiv: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
    - Transformer ã¨ Scaled Dot-Product Attention ã®ä¸€æ¬¡æ–‡çŒ®ã€‚

### MoEï¼ˆMixture of Expertsï¼‰

- Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. *ICLR 2017*. arXiv: [arXiv:1701.06538](https://arxiv.org/abs/1701.06538)
    - ç¾ä»£çš„MoEã®åŸºç¤è«–æ–‡ã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨æ¡ä»¶ä»˜ãè¨ˆç®—ã®è¨­è¨ˆã€‚
- Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. *JMLR*, 23(120), 1â€“39. arXiv: [arXiv:2101.03961](https://arxiv.org/abs/2101.03961)
    - MoEã®è¨“ç·´å®‰å®šåŒ–æŠ€è¡“ï¼ˆLoad Balancing Lossç­‰ï¼‰ã‚’æ‰±ã†ã€‚
